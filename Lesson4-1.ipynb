{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Introduction to Python\n",
    "================================\n",
    "\n",
    "Lesson 3 - Part1\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "\n",
    "In this lesson we will explore a little more in dept some aspects of Keras and neural networks.\n",
    "The topis that we'll cover in this lesson are:\n",
    "\n",
    "  - Dropout\n",
    "  - Grid Search\n",
    "  - \n",
    "  \n",
    "But before starting with the real Lesson let's make a little bit of advertising. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Advertising\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keep in touch #1\n",
    "\n",
    "If some of you are interested in a stage/work as data scientist in Leroy Merlin please contact me at:\n",
    "\n",
    "  - marco.saletta@leroymerlin.it\n",
    "\n",
    "Now it's being created a data science unit and maybe we can work together.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keep in touch #2\n",
    "\n",
    "The datascience environment (and the media/digital in general) in Milan is very small place and contacts are very important.\n",
    "\n",
    "If you are looking for a looking for a job, and you are not interested in Leroy Merlin, maybe let me know at\n",
    "\n",
    "  - marco.saletta@gmail.com\n",
    "\n",
    "Often someone asks me if I know a datascientis who is looking for a job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keep in touch #3\n",
    "\n",
    "In Milan there are many events related to datascience that are good for keeping up to date and being involved in the community.\n",
    "\n",
    "The two I want to address to you are:\n",
    "\n",
    "  - [Data Beers Milan](https://www.meetup.com/it-IT/DataBeers-Milano/?_cookie-check=ma6kEV7Q72i7FAiv): free beers and funny datascience talks\n",
    "  - [Data Science Milan](http://datasciencemilan.org/): nice talks with industrial sponsors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with the lesson\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Droptout\n",
    "\n",
    "Dropout is a method often used on neural networks in order to avoid/reduce orverfitting.\n",
    "\n",
    "The idea is to randomly shut down some neurons at each iteration in order to make them less relevant for the network.   \n",
    "\n",
    "For this dropout is a layer itself, as you can see from the [Keras docs](https://keras.io/layers/core/), that we put on top of another layer.\n",
    "\n",
    "Let's see how we can improve the network that we have created for the **mnist** classification task last lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Re-factoring\n",
    "\n",
    "In order to improve the code even more, we'll make a little bit of re-factoring and re-write it using Python Classes.\n",
    "\n",
    "**NOTE**: please note how the names of classes and function are written. As you can see \n",
    "\n",
    "  - every class has a name that starts with a **uppercase** letter\n",
    "  - every function has a name that starts with a **lowercase** letter\n",
    "  \n",
    "This is due to a style convention used in Python coding called [pep8](https://www.python.org/dev/peps/pep-0008/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "55000/55000 [==============================] - 8s 148us/step - loss: 0.2990 - acc: 0.9091 - mean_absolute_error: 0.0283 - val_loss: 0.1074 - val_acc: 0.9658 - val_mean_absolute_error: 0.0102\n",
      "Epoch 2/15\n",
      "55000/55000 [==============================] - 8s 153us/step - loss: 0.1233 - acc: 0.9629 - mean_absolute_error: 0.0115 - val_loss: 0.0826 - val_acc: 0.9755 - val_mean_absolute_error: 0.0080\n",
      "Epoch 3/15\n",
      "55000/55000 [==============================] - 8s 146us/step - loss: 0.0921 - acc: 0.9716 - mean_absolute_error: 0.0087 - val_loss: 0.0676 - val_acc: 0.9789 - val_mean_absolute_error: 0.0060\n",
      "Epoch 4/15\n",
      "55000/55000 [==============================] - 8s 139us/step - loss: 0.0733 - acc: 0.9772 - mean_absolute_error: 0.0069 - val_loss: 0.0897 - val_acc: 0.9741 - val_mean_absolute_error: 0.0069\n",
      "Epoch 5/15\n",
      "55000/55000 [==============================] - 8s 143us/step - loss: 0.0611 - acc: 0.9803 - mean_absolute_error: 0.0058 - val_loss: 0.0795 - val_acc: 0.9775 - val_mean_absolute_error: 0.0058\n",
      "Epoch 6/15\n",
      "55000/55000 [==============================] - 7s 132us/step - loss: 0.0549 - acc: 0.9829 - mean_absolute_error: 0.0051 - val_loss: 0.0706 - val_acc: 0.9799 - val_mean_absolute_error: 0.0051\n",
      "Epoch 7/15\n",
      "55000/55000 [==============================] - 8s 140us/step - loss: 0.0498 - acc: 0.9849 - mean_absolute_error: 0.0046 - val_loss: 0.0608 - val_acc: 0.9836 - val_mean_absolute_error: 0.0043\n",
      "Epoch 8/15\n",
      "55000/55000 [==============================] - 7s 131us/step - loss: 0.0406 - acc: 0.9875 - mean_absolute_error: 0.0039 - val_loss: 0.0669 - val_acc: 0.9815 - val_mean_absolute_error: 0.0045\n",
      "Epoch 9/15\n",
      "55000/55000 [==============================] - 8s 150us/step - loss: 0.0386 - acc: 0.9878 - mean_absolute_error: 0.0036 - val_loss: 0.0666 - val_acc: 0.9834 - val_mean_absolute_error: 0.0040\n",
      "Epoch 10/15\n",
      "55000/55000 [==============================] - 8s 137us/step - loss: 0.0390 - acc: 0.9877 - mean_absolute_error: 0.0036 - val_loss: 0.0834 - val_acc: 0.9782 - val_mean_absolute_error: 0.0054\n",
      "Epoch 11/15\n",
      "55000/55000 [==============================] - 8s 154us/step - loss: 0.0334 - acc: 0.9893 - mean_absolute_error: 0.0031 - val_loss: 0.0586 - val_acc: 0.9838 - val_mean_absolute_error: 0.0043\n",
      "Epoch 12/15\n",
      "55000/55000 [==============================] - 7s 134us/step - loss: 0.0315 - acc: 0.9895 - mean_absolute_error: 0.0030 - val_loss: 0.0681 - val_acc: 0.9825 - val_mean_absolute_error: 0.0044\n",
      "Epoch 13/15\n",
      "55000/55000 [==============================] - 7s 135us/step - loss: 0.0301 - acc: 0.9910 - mean_absolute_error: 0.0027 - val_loss: 0.0627 - val_acc: 0.9837 - val_mean_absolute_error: 0.0042\n",
      "Epoch 14/15\n",
      "55000/55000 [==============================] - 8s 139us/step - loss: 0.0275 - acc: 0.9913 - mean_absolute_error: 0.0025 - val_loss: 0.0668 - val_acc: 0.9831 - val_mean_absolute_error: 0.0039\n",
      "Epoch 15/15\n",
      "55000/55000 [==============================] - 8s 137us/step - loss: 0.0278 - acc: 0.9915 - mean_absolute_error: 0.0025 - val_loss: 0.0646 - val_acc: 0.9836 - val_mean_absolute_error: 0.0041\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "\n",
      "acc: 98.36%\n",
      "['loss', 'acc', 'mean_absolute_error']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import advanced_activations\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 15\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "metrics = ['accuracy','mae']\n",
    "\n",
    "\n",
    "class PrepareVariabiles:\n",
    "    def __init__(self):\n",
    "        self.mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "        \n",
    "        \n",
    "    def splitDatset(self):\n",
    "        X = self.mnist.train.images\n",
    "        Y = self.mnist.train.labels\n",
    "        X_test = self.mnist.test.images\n",
    "        Y_test = self.mnist.test.labels\n",
    "        return X,Y,X_test,Y_test\n",
    "    \n",
    "class CreateNN:\n",
    "    def __init__(self,**kargs):\n",
    "        self.activationFun = 'relu'\n",
    "        self.dropPerc = 0.25\n",
    "        self.X = kargs['x']\n",
    "        self.Y = kargs['y']\n",
    "        self.X_test = kargs['xt']\n",
    "        self.Y_test = kargs['yt']\n",
    "        \n",
    "        \n",
    "    def modelDefinition(self,useDropout=False):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(num_input, input_dim=num_input,activation=self.activationFun))\n",
    "        if useDropout:\n",
    "            self.model.add(Dropout(self.dropPerc))\n",
    "        self.model.add(Dense(n_hidden_1,activation=self.activationFun))\n",
    "        if useDropout:\n",
    "            self.model.add(Dropout(self.dropPerc))\n",
    "        self.model.add(Dense(n_hidden_2,activation=self.activationFun))\n",
    "        if useDropout:\n",
    "            self.model.add(Dropout(self.dropPerc))\n",
    "        self.model.add(Dense(num_classes,activation='softmax'))\n",
    "        \n",
    "    def modelCompile(self):\n",
    "        adam = Adam(lr=learning_rate)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=adam,metrics=metrics)\n",
    "        \n",
    "    def modelEval(self):\n",
    "        history = self.model.fit(self.X, self.Y, epochs=num_steps, batch_size=batch_size,validation_data=(self.X_test,self.Y_test))#validation_split=0.05\n",
    "        scores = self.model.evaluate(self.X_test, self.Y_test)\n",
    "        return history,scores,self.model\n",
    "    \n",
    "class PlotGraphs:\n",
    "    def __init__(self,**kargs):\n",
    "        history= kargs['h']\n",
    "        self.history_dict = history.history\n",
    "        self.metList = []\n",
    "        for cur_key in history.history.keys():\n",
    "            if cur_key.find('val')!=0:\n",
    "                self.metList.append(cur_key)\n",
    "        print(self.metList)\n",
    "        self.lenList = len(self.metList)\n",
    "    \n",
    "    def plotResults(self):\n",
    "        plotPos = 1\n",
    "        for cur_met in self.metList:\n",
    "            cur_values = self.history_dict[cur_met]\n",
    "            cur_val = self.history_dict['val_%s'%cur_met]\n",
    "            epochs = range(1, len(cur_values) + 1)\n",
    "            plt.subplot(self.lenList, 1, plotPos)\n",
    "            plt.plot(epochs, cur_values, 'ro')\n",
    "            plt.plot(epochs, cur_val, 'b+')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel(cur_met)\n",
    "            plotPos += 1\n",
    "\n",
    "\n",
    "def main():\n",
    "    pv = PrepareVariabiles()\n",
    "    X, Y, X_test, Y_test = pv.splitDatset()\n",
    "    cnn = CreateNN(x=X,y=Y,xt=X_test,yt=Y_test)\n",
    "    cnn.modelDefinition(useDropout=True)#useDropout=True\n",
    "    cnn.modelCompile()\n",
    "    history, scores, model = cnn.modelEval()\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    pg = PlotGraphs(h=history)\n",
    "    pg.plotResults()\n",
    "    return history\n",
    "history = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "acc\n",
      "mean_absolute_error\n"
     ]
    }
   ],
   "source": [
    "history.history.keys()\n",
    "for cur_key in history.history.keys():\n",
    "    if cur_key.find('val')!=0:\n",
    "        print(cur_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid Search \n",
    "\n",
    "The `grid search` is the names the `sklearn` uses to indicate the **hyper parameter optimization** ([form wiki](https://en.wikipedia.org/wiki/Hyperparameter_optimization)):\n",
    "\n",
    "*In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.\n",
    "The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance.*\n",
    "\n",
    "The package that we'll use is `GridSearchCV` from `sklearn.model_selection` ([documentation page](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).\n",
    "\n",
    "As you can see at main Sklearn's page about [Tuning Hyper Parameters](http://scikit-learn.org/stable/modules/grid_search.html), this is just one of the possibility that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid Search\n",
    "\n",
    "In order to make highlight some implementation's difference and make the code more general, we'll see how to use the grid search over two different kind of models:\n",
    "\n",
    "  - NN (`keras`)\n",
    "  - Random Forest (`sklearn`)\n",
    "  \n",
    "**NOTE**: Since all `sklearn` models have the same structure, the same code can be used for other algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-14 12:28:01,172 : INFO : START\n",
      "2018-10-14 12:28:01,173 : INFO : READING THE HYPER PARAMETERS FOR THE SELECTED MODEL\n",
      "2018-10-14 12:28:01,175 : INFO : DEFINING THE SCORING FUNCTION FOR THE GRID SEARCH\n",
      "2018-10-14 12:28:01,176 : INFO : LOADING THE DATA SET\n",
      "2018-10-14 12:28:01,177 : INFO : READING data/target.csv\n",
      "2018-10-14 12:28:01,337 : INFO : LOADED DATASET WITH SHAPE (110000, 8) AND COLUMUNS Index(['customer_id', 'card_tenure', 'risk_score', 'num_promoted', 'avg_bal',\n",
      "       'geo_group', 'res_type', 'Unnamed: 7'],\n",
      "      dtype='object')\n",
      "2018-10-14 12:28:01,370 : INFO : READING data/promoted.csv\n",
      "2018-10-14 12:28:01,409 : INFO : LOADED DATASET WITH SHAPE (25000, 8) AND COLUMUNS Index(['customer_id', 'resp', 'card_tenure', 'risk_score', 'num_promoted',\n",
      "       'avg_bal', 'geo_group', 'res_type'],\n",
      "      dtype='object')\n",
      "2018-10-14 12:28:01,430 : INFO : REMOVING ROWS WITH NA\n",
      "2018-10-14 12:28:01,431 : INFO : NROWS BEFORE REMOVING NA 25000\n",
      "2018-10-14 12:28:01,442 : INFO : NROWS AFTER REMOVING NA 22400\n",
      "2018-10-14 12:28:01,445 : INFO : SCALING OF NUMERIC COLUMNS\n",
      "2018-10-14 12:28:01,467 : INFO : CONSIDERING LEVELS FOR CATEGORICAL COLUMNS\n",
      "2018-10-14 12:28:01,471 : INFO : AFTER PREPROCESSING X_train HAS COLUMUNS Index(['card_tenure', 'risk_score', 'num_promoted', 'avg_bal', 'geo_group',\n",
      "       'res_type'],\n",
      "      dtype='object') AND TYPES card_tenure     float64\n",
      "risk_score      float64\n",
      "num_promoted    float64\n",
      "avg_bal           int16\n",
      "geo_group          int8\n",
      "res_type           int8\n",
      "dtype: object\n",
      "2018-10-14 12:28:01,474 : INFO : CREATION OF THE MODEL\n",
      "2018-10-14 12:28:01,476 : INFO : START GRID SEARCH\n",
      "2018-10-14 12:28:01,476 : INFO : START GRID SEARCH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reading          card_tenure     risk_score   num_promoted  Unnamed: 7\n",
      "count  107792.000000  110000.000000  110000.000000         0.0\n",
      "mean      138.956564     655.571482       0.006782         NaN\n",
      "std        67.433081      81.252328       0.082183         NaN\n",
      "min        12.000000     520.000000       0.000000         NaN\n",
      "25%        91.000000     600.000000       0.000000         NaN\n",
      "50%       135.000000     678.000000       0.000000         NaN\n",
      "75%       179.000000     720.000000       0.000000         NaN\n",
      "max       641.000000     760.000000       2.000000         NaN\n",
      "After reading                resp   card_tenure    risk_score  num_promoted\n",
      "count  25000.000000  24515.000000  25000.000000  25000.000000\n",
      "mean       0.068640    139.491617    655.091680      0.007000\n",
      "std        0.252846     66.998010     81.315116      0.083374\n",
      "min        0.000000      0.000000    520.000000      0.000000\n",
      "25%        0.000000     95.000000    599.000000      0.000000\n",
      "50%        0.000000    135.000000    677.000000      0.000000\n",
      "75%        0.000000    179.000000    719.000000      0.000000\n",
      "max        1.000000    641.000000    760.000000      1.000000\n",
      "X        card_tenure  risk_score  num_promoted  avg_bal  geo_group  res_type\n",
      "0         0.223089    0.000000           0.0     2895          0         4\n",
      "1         0.141966    0.725000           0.0     2032          1         1\n",
      "3         0.229329    0.650000           0.0     2170          0         1\n",
      "4         0.346334    0.820833           0.0     2353          2         1\n",
      "5         0.254290    0.083333           0.0     1813          0         4\n",
      "6         0.191888    0.933333           0.0      637          0         3\n",
      "7         0.129485    0.566667           0.0     1905          1         1\n",
      "9         0.266771    0.000000           0.0     1392          0         4\n",
      "10        0.327613    0.766667           0.0     2763          2         1\n",
      "11        0.223089    0.558333           0.0     1996          0         1\n",
      "12        0.117005    0.720833           0.0     2698          1         1\n",
      "13        0.290172    0.654167           0.0     1996          0         1\n",
      "14        0.235569    0.733333           0.0      383          0         1\n",
      "15        0.179407    0.612500           0.0      833          1         1\n",
      "16        0.248050    0.712500           0.0     1649          0         1\n",
      "17        0.352574    0.745833           0.0     1454          2         1\n",
      "18        0.254290    0.462500           0.0     1885          0         0\n",
      "19        0.166927    0.987500           0.0     2281          1         3\n",
      "20        0.141966    0.812500           0.0     1048          1         1\n",
      "21        0.229329    0.679167           0.0     1438          0         1\n",
      "22        0.117005    0.945833           0.0     1578          1         3\n",
      "23        0.383775    1.000000           0.0      845          2         3\n",
      "24        0.358814    0.000000           0.0     2542          2         4\n",
      "25        0.995320    0.000000           0.0     3219          3         4\n",
      "26        0.273011    0.737500           0.0      637          0         1\n",
      "27        0.191888    0.000000           0.0     1813          0         4\n",
      "28        0.241810    0.554167           0.0     1040          0         1\n",
      "29        0.254290    0.000000           0.0     1217          0         4\n",
      "30        0.135725    0.395833           0.0     1776          1         2\n",
      "31        0.235569    0.866667           0.0     1631          0         3\n",
      "...            ...         ...           ...      ...        ...       ...\n",
      "24967     0.235569    0.000000           0.0     1863          0         4\n",
      "24968     0.383775    0.770833           0.0     2342          2         1\n",
      "24970     0.327613    1.000000           0.0     1193          2         3\n",
      "24971     0.254290    0.608333           0.0     1076          0         1\n",
      "24972     0.266771    0.429167           0.0     2232          0         0\n",
      "24973     0.346334    0.266667           0.0     2346          2         2\n",
      "24974     0.068643    0.750000           0.0     2582          3         1\n",
      "24975     0.198128    0.866667           0.0     1905          0         3\n",
      "24976     0.141966    0.000000           0.0     1618          1         4\n",
      "24977     0.117005    0.920833           0.0     2389          1         3\n",
      "24978     0.166927    0.108333           0.0     2232          1         4\n",
      "24979     0.266771    0.704167           0.0     2148          0         1\n",
      "24980     0.273011    0.000000           0.0      837          0         4\n",
      "24981     0.333853    0.550000           0.0     2546          2         0\n",
      "24982     0.819033    0.000000           0.0     2509          3         4\n",
      "24983     0.141966    0.133333           0.0     2637          1         4\n",
      "24985     0.141966    0.670833           0.0     1702          1         1\n",
      "24986     0.129485    0.770833           0.0     2492          1         1\n",
      "24987     0.296412    0.000000           0.0     2661          0         4\n",
      "24988     0.098284    0.729167           0.0     2684          3         1\n",
      "24989     0.154446    0.608333           0.0     1236          1         1\n",
      "24990     0.110764    0.266667           0.0     2528          3         2\n",
      "24992     0.185647    0.441667           0.0     1100          1         0\n",
      "24993     0.223089    0.550000           0.0     1722          0         0\n",
      "24994     0.229329    0.000000           0.0     2270          0         4\n",
      "24995     0.266771    1.000000           0.0     2382          0         3\n",
      "24996     0.254290    0.845833           0.0     2032          0         1\n",
      "24997     0.283931    0.641667           0.0     2814          0         1\n",
      "24998     0.383775    0.675000           0.0     2270          2         1\n",
      "24999     0.173167    0.850000           0.0     2236          1         3\n",
      "\n",
      "[22400 rows x 6 columns]\n",
      "Y 0        0\n",
      "1        0\n",
      "3        0\n",
      "4        0\n",
      "5        0\n",
      "6        0\n",
      "7        0\n",
      "9        0\n",
      "10       0\n",
      "11       0\n",
      "12       0\n",
      "13       0\n",
      "14       0\n",
      "15       0\n",
      "16       0\n",
      "17       0\n",
      "18       0\n",
      "19       0\n",
      "20       0\n",
      "21       0\n",
      "22       0\n",
      "23       0\n",
      "24       0\n",
      "25       0\n",
      "26       0\n",
      "27       0\n",
      "28       0\n",
      "29       0\n",
      "30       0\n",
      "31       0\n",
      "        ..\n",
      "24967    0\n",
      "24968    0\n",
      "24970    0\n",
      "24971    0\n",
      "24972    0\n",
      "24973    0\n",
      "24974    0\n",
      "24975    0\n",
      "24976    0\n",
      "24977    0\n",
      "24978    1\n",
      "24979    0\n",
      "24980    0\n",
      "24981    0\n",
      "24982    0\n",
      "24983    0\n",
      "24985    0\n",
      "24986    0\n",
      "24987    0\n",
      "24988    1\n",
      "24989    0\n",
      "24990    0\n",
      "24992    0\n",
      "24993    0\n",
      "24994    1\n",
      "24995    0\n",
      "24996    0\n",
      "24997    0\n",
      "24998    0\n",
      "24999    0\n",
      "Name: resp, Length: 22400, dtype: int64\n",
      "Epoch 1/15\n",
      "Epoch 1/10\n",
      "Epoch 1/10\n",
      "Epoch 1/10\n",
      "14933/14933 [==============================] - 3s 222us/step - loss: 1.3957 - acc: 0.9112\n",
      "14760/14933 [============================>.] - ETA: 0s - loss: 1.3959 - acc: 0.9112Epoch 2/15\n",
      "14933/14933 [==============================] - 3s 222us/step - loss: 1.3950 - acc: 0.9112\n",
      "Epoch 2/10\n",
      "14933/14933 [==============================] - 3s 227us/step - loss: 1.5152 - acc: 0.9042\n",
      "Epoch 2/10\n",
      "14934/14934 [==============================] - 3s 226us/step - loss: 1.4406 - acc: 0.9085\n",
      "Epoch 2/10\n",
      "14933/14933 [==============================] - 3s 186us/step - loss: 1.0728 - acc: 0.9328\n",
      "Epoch 3/15\n",
      "14933/14933 [==============================] - 3s 183us/step - loss: 1.1359 - acc: 0.9283\n",
      "Epoch 3/10\n",
      "14933/14933 [==============================] - 3s 187us/step - loss: 1.0735 - acc: 0.9328\n",
      "Epoch 3/10\n",
      "14934/14934 [==============================] - 3s 187us/step - loss: 1.0722 - acc: 0.9322\n",
      "Epoch 3/10\n",
      "14933/14933 [==============================] - 2s 164us/step - loss: 1.1427 - acc: 0.9287\n",
      "Epoch 4/10\n",
      "14933/14933 [==============================] - 2s 165us/step - loss: 1.0729 - acc: 0.9328\n",
      "Epoch 4/10\n",
      "14933/14933 [==============================] - 3s 171us/step - loss: 1.0709 - acc: 0.9328\n",
      "Epoch 4/15\n",
      "14934/14934 [==============================] - 3s 172us/step - loss: 1.0750 - acc: 0.9327\n",
      "Epoch 4/10\n",
      "14933/14933 [==============================] - 3s 168us/step - loss: 1.0755 - acc: 0.9323\n",
      "Epoch 5/10\n",
      "14933/14933 [==============================] - 3s 171us/step - loss: 1.1426 - acc: 0.9287\n",
      "Epoch 5/10\n",
      "14933/14933 [==============================] - 3s 168us/step - loss: 1.0738 - acc: 0.9328\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14934/14934 [==============================] - 3s 174us/step - loss: 1.0740 - acc: 0.9327\n",
      "Epoch 5/10\n",
      "14933/14933 [==============================] - 3s 185us/step - loss: 1.0735 - acc: 0.9328\n",
      "Epoch 6/15\n",
      "14933/14933 [==============================] - 3s 189us/step - loss: 1.1424 - acc: 0.9287\n",
      "Epoch 6/10\n",
      "14933/14933 [==============================] - 3s 194us/step - loss: 1.0745 - acc: 0.9328\n",
      "Epoch 6/10\n",
      "14934/14934 [==============================] - 3s 190us/step - loss: 1.0716 - acc: 0.9327\n",
      "Epoch 6/10\n",
      "14933/14933 [==============================] - 3s 187us/step - loss: 2.4191 - acc: 0.8475\n",
      "Epoch 7/15\n",
      "14933/14933 [==============================] - 3s 183us/step - loss: 1.0744 - acc: 0.9328\n",
      "Epoch 7/10\n",
      "14933/14933 [==============================] - 3s 189us/step - loss: 1.1441 - acc: 0.9285\n",
      "Epoch 7/10\n",
      "14934/14934 [==============================] - 3s 193us/step - loss: 1.0895 - acc: 0.9314\n",
      "Epoch 7/10\n",
      "14933/14933 [==============================] - 3s 184us/step - loss: 1.0741 - acc: 0.9328\n",
      "Epoch 8/15\n",
      "14933/14933 [==============================] - 3s 181us/step - loss: 1.0742 - acc: 0.9328\n",
      "Epoch 8/10\n",
      "14933/14933 [==============================] - 3s 184us/step - loss: 1.1421 - acc: 0.9287\n",
      "Epoch 8/10\n",
      "14934/14934 [==============================] - 3s 189us/step - loss: 1.0733 - acc: 0.9327\n",
      "Epoch 8/10\n",
      "14933/14933 [==============================] - 3s 178us/step - loss: 1.0737 - acc: 0.9328\n",
      "Epoch 9/10\n",
      "14933/14933 [==============================] - 3s 176us/step - loss: 1.1417 - acc: 0.9287\n",
      "14680/14933 [============================>.] - ETA: 0s - loss: 1.0833 - acc: 0.9322Epoch 9/10\n",
      "14933/14933 [==============================] - 3s 185us/step - loss: 1.0735 - acc: 0.9328\n",
      "Epoch 9/15\n",
      "14934/14934 [==============================] - 3s 178us/step - loss: 1.0701 - acc: 0.9327\n",
      "Epoch 9/10\n",
      "14933/14933 [==============================] - 3s 177us/step - loss: 1.9347 - acc: 0.8780\n",
      "Epoch 10/10\n",
      "14933/14933 [==============================] - 3s 177us/step - loss: 1.7615 - acc: 0.8892\n",
      "Epoch 10/10\n",
      "14933/14933 [==============================] - 3s 176us/step - loss: 1.0726 - acc: 0.9328\n",
      "Epoch 10/15\n",
      "14934/14934 [==============================] - 3s 175us/step - loss: 1.0692 - acc: 0.9326\n",
      "Epoch 10/10\n",
      "14933/14933 [==============================] - 3s 186us/step - loss: 1.0740 - acc: 0.9328\n",
      "14933/14933 [==============================] - 3s 186us/step - loss: 1.0868 - acc: 0.9317\n",
      "Epoch 11/15\n",
      "14933/14933 [==============================] - 3s 191us/step - loss: 1.1425 - acc: 0.9287\n",
      "14934/14934 [==============================] - 3s 187us/step - loss: 1.0700 - acc: 0.9327\n",
      "14370/14933 [===========================>..] - ETA: 0s - loss: 1.0703 - acc: 0.9331Epoch 1/15\n",
      "14933/14933 [==============================] - 2s 142us/step - loss: 1.0733 - acc: 0.9328\n",
      "Epoch 12/15\n",
      "   10/14933 [..............................] - ETA: 4s - loss: 3.2236 - acc: 0.8000Epoch 1/15\n",
      " 1640/14933 [==>...........................] - ETA: 2s - loss: 0.9859 - acc: 0.9384Epoch 1/10\n",
      "14933/14933 [==============================] - 2s 160us/step - loss: 1.0730 - acc: 0.9328\n",
      "Epoch 13/15\n",
      "14933/14933 [==============================] - 3s 182us/step - loss: 1.0757 - acc: 0.9328\n",
      "Epoch 2/10\n",
      "14933/14933 [==============================] - 4s 260us/step - loss: 1.1441 - acc: 0.9287\n",
      "Epoch 2/15\n",
      "14934/14934 [==============================] - 4s 255us/step - loss: 1.0778 - acc: 0.9327\n",
      "Epoch 2/15\n",
      "14933/14933 [==============================] - 1s 97us/step - loss: 1.0758 - acc: 0.9328\n",
      "Epoch 3/10\n",
      "14933/14933 [==============================] - 3s 178us/step - loss: 1.0711 - acc: 0.9324\n",
      "Epoch 14/15\n",
      "14933/14933 [==============================] - 1s 90us/step - loss: 1.0807 - acc: 0.9317\n",
      "Epoch 4/10\n",
      "14933/14933 [==============================] - 3s 186us/step - loss: 1.1440 - acc: 0.9287\n",
      "Epoch 3/15\n",
      "14934/14934 [==============================] - 3s 186us/step - loss: 1.0776 - acc: 0.9327\n",
      "Epoch 3/15\n",
      "14933/14933 [==============================] - 1s 93us/step - loss: 1.0752 - acc: 0.9328\n",
      " 3270/14934 [=====>........................] - ETA: 1s - loss: 1.0526 - acc: 0.9343Epoch 5/10\n",
      "14933/14933 [==============================] - 3s 175us/step - loss: 1.0709 - acc: 0.9328\n",
      "Epoch 15/15\n",
      "14933/14933 [==============================] - 1s 93us/step - loss: 1.0752 - acc: 0.9328\n",
      "Epoch 6/10\n",
      "14933/14933 [==============================] - 3s 178us/step - loss: 1.1439 - acc: 0.9287\n",
      "Epoch 4/15\n",
      "14934/14934 [==============================] - 3s 179us/step - loss: 1.0776 - acc: 0.9327\n",
      "Epoch 4/15\n",
      "14933/14933 [==============================] - 1s 94us/step - loss: 1.0752 - acc: 0.9328\n",
      "13120/14933 [=========================>....] - ETA: 0s - loss: 1.0742 - acc: 0.9321Epoch 7/10\n",
      "14933/14933 [==============================] - 3s 177us/step - loss: 1.0708 - acc: 0.9323\n",
      "14933/14933 [==============================] - 1s 91us/step - loss: 1.0752 - acc: 0.9328\n",
      "Epoch 8/10\n",
      "14934/14934 [==============================] - 3s 181us/step - loss: 1.0775 - acc: 0.9327\n",
      "Epoch 5/15\n",
      "14933/14933 [==============================] - 3s 184us/step - loss: 1.1437 - acc: 0.9287\n",
      "Epoch 5/15\n",
      "14933/14933 [==============================] - 1s 88us/step - loss: 1.0752 - acc: 0.9328\n",
      "Epoch 9/10\n",
      " 9100/14934 [=================>............] - ETA: 1s - loss: 1.0986 - acc: 0.9314Epoch 1/10\n",
      "14933/14933 [==============================] - 1s 87us/step - loss: 1.0752 - acc: 0.9328\n",
      "Epoch 10/10\n",
      "14934/14934 [==============================] - 3s 172us/step - loss: 1.0775 - acc: 0.9327\n",
      "Epoch 6/15\n",
      "14933/14933 [==============================] - 3s 174us/step - loss: 1.1439 - acc: 0.9287\n",
      "Epoch 6/15\n",
      "14933/14933 [==============================] - 1s 87us/step - loss: 1.0752 - acc: 0.9328+\n",
      "14933/14933 [==============================] - 3s 196us/step - loss: 1.6328 - acc: 0.8937\n",
      "Epoch 2/10\n",
      "14934/14934 [==============================] - 3s 170us/step - loss: 1.0774 - acc: 0.9327\n",
      "Epoch 7/15\n",
      "14933/14933 [==============================] - 3s 168us/step - loss: 1.1438 - acc: 0.9287\n",
      "Epoch 7/15\n",
      " 3240/14934 [=====>........................] - ETA: 1s - loss: 1.1018 - acc: 0.9312Epoch 1/10\n",
      "14933/14933 [==============================] - 1s 81us/step - loss: 1.1426 - acc: 0.9287\n",
      "Epoch 3/10\n",
      "14933/14933 [==============================] - 1s 79us/step - loss: 1.1436 - acc: 0.9279\n",
      "Epoch 4/10\n",
      "14934/14934 [==============================] - 2s 163us/step - loss: 1.0775 - acc: 0.9327\n",
      " 8980/14934 [=================>............] - ETA: 1s - loss: 1.3851 - acc: 0.9124Epoch 8/15\n",
      "14933/14933 [==============================] - 2s 162us/step - loss: 1.1439 - acc: 0.9287\n",
      "Epoch 8/15\n",
      "14934/14934 [==============================] - 2s 165us/step - loss: 1.2591 - acc: 0.9205\n",
      "Epoch 2/10\n",
      "14933/14933 [==============================] - 1s 89us/step - loss: 1.1439 - acc: 0.9287\n",
      "Epoch 5/10\n",
      "14934/14934 [==============================] - 1s 94us/step - loss: 12.1496 - acc: 0.2353\n",
      "Epoch 3/10\n",
      "14933/14933 [==============================] - 1s 92us/step - loss: 1.1439 - acc: 0.9287\n",
      " 1640/14934 [==>...........................] - ETA: 1s - loss: 7.4701 - acc: 0.5287 Epoch 6/10\n",
      "14934/14934 [==============================] - 3s 181us/step - loss: 1.0774 - acc: 0.9327\n",
      "Epoch 9/15\n",
      "14933/14933 [==============================] - 3s 187us/step - loss: 1.1439 - acc: 0.9287\n",
      "Epoch 9/15\n",
      "14934/14934 [==============================] - 1s 98us/step - loss: 1.7661 - acc: 0.8892\n",
      "Epoch 4/10\n",
      "14933/14933 [==============================] - 1s 94us/step - loss: 1.1439 - acc: 0.9287\n",
      "Epoch 7/10\n",
      "14934/14934 [==============================] - 1s 96us/step - loss: 1.0785 - acc: 0.9327\n",
      "Epoch 5/10\n",
      "14933/14933 [==============================] - 1s 95us/step - loss: 1.1439 - acc: 0.9287\n",
      "  540/14934 [>.............................] - ETA: 1s - loss: 0.9851 - acc: 0.9389Epoch 8/10\n",
      "14934/14934 [==============================] - 3s 190us/step - loss: 1.0774 - acc: 0.9327\n",
      "Epoch 10/15\n",
      "14933/14933 [==============================] - 3s 193us/step - loss: 1.1542 - acc: 0.9281\n",
      "Epoch 10/15\n",
      "14933/14933 [==============================] - 1s 95us/step - loss: 1.1439 - acc: 0.9287\n",
      "Epoch 9/10\n",
      "14934/14934 [==============================] - 2s 104us/step - loss: 1.0780 - acc: 0.9327\n",
      "Epoch 6/10\n",
      "14933/14933 [==============================] - 1s 93us/step - loss: 1.1439 - acc: 0.9287\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14934/14934 [==============================] - 1s 95us/step - loss: 1.0776 - acc: 0.9327\n",
      "Epoch 7/10\n",
      "14934/14934 [==============================] - 3s 194us/step - loss: 1.0774 - acc: 0.9327\n",
      "Epoch 11/15\n",
      "14933/14933 [==============================] - 3s 187us/step - loss: 1.1436 - acc: 0.9287\n",
      " 5560/14934 [==========>...................] - ETA: 1s - loss: 1.0877 - acc: 0.9318Epoch 11/15\n",
      "14933/14933 [==============================] - 2s 110us/step - loss: 1.1438 - acc: 0.9287\n",
      "14934/14934 [==============================] - 2s 110us/step - loss: 1.0775 - acc: 0.9327\n",
      "Epoch 8/10\n",
      "14934/14934 [==============================] - 2s 101us/step - loss: 1.0774 - acc: 0.9327\n",
      "Epoch 9/10\n",
      "14933/14933 [==============================] - 3s 197us/step - loss: 1.1436 - acc: 0.9287\n",
      "Epoch 12/15\n",
      "14934/14934 [==============================] - 3s 202us/step - loss: 1.0774 - acc: 0.9327\n",
      "Epoch 12/15\n",
      " 3900/14934 [======>.......................] - ETA: 1s - loss: 1.1421 - acc: 0.9290Epoch 1/15\n",
      "14934/14934 [==============================] - 1s 91us/step - loss: 1.0774 - acc: 0.9327\n",
      "Epoch 10/10\n",
      "14934/14934 [==============================] - 1s 99us/step - loss: 1.0773 - acc: 0.9327\n",
      "14933/14933 [==============================] - 3s 186us/step - loss: 1.1436 - acc: 0.9287\n",
      "Epoch 13/15\n",
      "14934/14934 [==============================] - 3s 184us/step - loss: 1.0774 - acc: 0.9327\n",
      " 9000/14933 [=================>............] - ETA: 1s - loss: 1.2045 - acc: 0.9221Epoch 13/15\n",
      "14933/14933 [==============================] - 3s 187us/step - loss: 1.1504 - acc: 0.9265\n",
      " 3180/14934 [=====>........................] - ETA: 2s - loss: 1.0391 - acc: 0.9343Epoch 2/15\n",
      "14933/14933 [==============================] - 2s 102us/step - loss: 1.5487 - acc: 0.9018\n",
      "10940/14934 [====================>.........] - ETA: 0s - loss: 1.0470 - acc: 0.9346Epoch 3/15\n",
      "14933/14933 [==============================] - 3s 201us/step - loss: 8.2796 - acc: 0.4795\n",
      "Epoch 14/15\n",
      "14934/14934 [==============================] - 3s 200us/step - loss: 1.0773 - acc: 0.9327\n",
      "Epoch 14/15\n",
      "   10/14934 [..............................] - ETA: 3s - loss: 1.0000e-07 - acc: 1.0000Epoch 1/15\n",
      "14933/14933 [==============================] - 2s 101us/step - loss: 1.0989 - acc: 0.9306\n",
      "Epoch 4/15\n",
      "14933/14933 [==============================] - 2s 104us/step - loss: 1.0750 - acc: 0.9328\n",
      "Epoch 5/15\n",
      "14933/14933 [==============================] - 3s 195us/step - loss: 1.2740 - acc: 0.9109\n",
      "14933/14933 [==============================] - 3s 197us/step - loss: 1.1437 - acc: 0.9287\n",
      "Epoch 15/15\n",
      "Epoch 2/15\n",
      "14934/14934 [==============================] - 3s 200us/step - loss: 1.0773 - acc: 0.9327\n",
      "  640/14933 [>.............................] - ETA: 1s - loss: 1.2463 - acc: 0.9203Epoch 15/15\n",
      "14933/14933 [==============================] - 2s 100us/step - loss: 1.0750 - acc: 0.9328\n",
      "Epoch 6/15\n",
      "14933/14933 [==============================] - 2s 108us/step - loss: 1.1448 - acc: 0.9259\n",
      "Epoch 3/15\n",
      "14933/14933 [==============================] - 2s 105us/step - loss: 1.0749 - acc: 0.9328\n",
      "Epoch 7/15\n",
      "14933/14933 [==============================] - 3s 202us/step - loss: 1.1435 - acc: 0.9287\n",
      "14934/14934 [==============================] - 3s 202us/step - loss: 1.0773 - acc: 0.9327\n",
      "14933/14933 [==============================] - 2s 107us/step - loss: 1.1845 - acc: 0.9251\n",
      "Epoch 4/15\n",
      "14933/14933 [==============================] - 1s 90us/step - loss: 1.0749 - acc: 0.9328\n",
      "Epoch 8/15\n",
      "14933/14933 [==============================] - 1s 84us/step - loss: 1.1440 - acc: 0.9287\n",
      "Epoch 5/15\n",
      "14933/14933 [==============================] - 1s 84us/step - loss: 1.0749 - acc: 0.9328\n",
      "Epoch 9/15\n",
      "13960/14933 [===========================>..] - ETA: 0s - loss: 1.1369 - acc: 0.9292Epoch 1/15\n",
      "14933/14933 [==============================] - 1s 84us/step - loss: 1.1440 - acc: 0.9287\n",
      "Epoch 6/15\n",
      "14933/14933 [==============================] - 1s 81us/step - loss: 1.0749 - acc: 0.9328\n",
      "Epoch 10/15\n",
      "14933/14933 [==============================] - 1s 85us/step - loss: 1.1440 - acc: 0.9287\n",
      "Epoch 7/15\n",
      "14933/14933 [==============================] - 1s 86us/step - loss: 1.0748 - acc: 0.9328\n",
      "Epoch 11/15\n",
      "14934/14934 [==============================] - 2s 156us/step - loss: 1.1552 - acc: 0.9258\n",
      "Epoch 2/15\n",
      "14933/14933 [==============================] - 1s 92us/step - loss: 1.1440 - acc: 0.9287\n",
      "Epoch 8/15\n",
      "14933/14933 [==============================] - 1s 89us/step - loss: 1.0746 - acc: 0.9328\n",
      "Epoch 12/15\n",
      "14934/14934 [==============================] - 1s 87us/step - loss: 1.0752 - acc: 0.9318\n",
      "Epoch 3/15\n",
      "14933/14933 [==============================] - 1s 86us/step - loss: 1.1440 - acc: 0.9287\n",
      "Epoch 9/15\n",
      "14933/14933 [==============================] - 1s 87us/step - loss: 1.0742 - acc: 0.9328\n",
      "Epoch 13/15\n",
      "14934/14934 [==============================] - 1s 86us/step - loss: 1.0735 - acc: 0.9327\n",
      "Epoch 4/15\n",
      "14933/14933 [==============================] - 1s 87us/step - loss: 1.1439 - acc: 0.9287\n",
      "Epoch 10/15\n",
      "14933/14933 [==============================] - 1s 81us/step - loss: 1.0931 - acc: 0.9315\n",
      "Epoch 14/15\n",
      "14934/14934 [==============================] - 1s 81us/step - loss: 1.0733 - acc: 0.9327\n",
      " 3860/14933 [======>.......................] - ETA: 0s - loss: 1.0431 - acc: 0.9347Epoch 5/15\n",
      "14933/14933 [==============================] - 1s 81us/step - loss: 1.1439 - acc: 0.9287\n",
      "Epoch 11/15\n",
      "14933/14933 [==============================] - 1s 80us/step - loss: 1.0750 - acc: 0.9328\n",
      "Epoch 15/15\n",
      "14934/14934 [==============================] - 1s 83us/step - loss: 1.0729 - acc: 0.9327\n",
      "Epoch 6/15\n",
      "14933/14933 [==============================] - 1s 87us/step - loss: 1.1439 - acc: 0.9287\n",
      "Epoch 12/15\n",
      "14933/14933 [==============================] - 1s 84us/step - loss: 1.0749 - acc: 0.9328\n",
      "14934/14934 [==============================] - 1s 80us/step - loss: 1.0723 - acc: 0.9327\n",
      "Epoch 7/15\n",
      "14933/14933 [==============================] - 1s 77us/step - loss: 1.1439 - acc: 0.9287\n",
      "Epoch 13/15\n",
      "14934/14934 [==============================] - 1s 76us/step - loss: 1.0721 - acc: 0.9317\n",
      "Epoch 8/15\n",
      "14933/14933 [==============================] - 1s 80us/step - loss: 1.1438 - acc: 0.9287\n",
      "Epoch 14/15\n",
      "14934/14934 [==============================] - 1s 81us/step - loss: 1.0771 - acc: 0.9327\n",
      "Epoch 9/15\n",
      "14933/14933 [==============================] - 1s 80us/step - loss: 1.1438 - acc: 0.9287\n",
      "Epoch 15/15\n",
      "14934/14934 [==============================] - 1s 80us/step - loss: 1.0770 - acc: 0.9327\n",
      "Epoch 10/15\n",
      "14933/14933 [==============================] - 1s 81us/step - loss: 1.1438 - acc: 0.9287\n",
      "14934/14934 [==============================] - 1s 66us/step - loss: 1.0770 - acc: 0.9327\n",
      "Epoch 11/15\n",
      "14934/14934 [==============================] - 1s 50us/step - loss: 1.0769 - acc: 0.9327\n",
      "Epoch 12/15\n",
      "14934/14934 [==============================] - 1s 52us/step - loss: 1.0768 - acc: 0.9327\n",
      "Epoch 13/15\n",
      "14934/14934 [==============================] - 1s 51us/step - loss: 1.0767 - acc: 0.9327\n",
      "Epoch 14/15\n",
      "14934/14934 [==============================] - 1s 51us/step - loss: 1.0764 - acc: 0.9327\n",
      "Epoch 15/15\n",
      "14934/14934 [==============================] - 1s 52us/step - loss: 1.0749 - acc: 0.9327\n",
      "Epoch 1/10\n",
      "22400/22400 [==============================] - 2s 100us/step - loss: 1.4494 - acc: 0.9087 1s - loss: 1.7687  - ETA: 0s - loss: 1.5109 - acc:\n",
      "Epoch 2/10\n",
      "22400/22400 [==============================] - 2s 92us/step - loss: 1.0968 - acc: 0.9314\n",
      "Epoch 3/10\n",
      "22400/22400 [==============================] - 2s 93us/step - loss: 3.7459 - acc: 0.7642\n",
      "Epoch 4/10\n",
      "22400/22400 [==============================] - 2s 90us/step - loss: 1.0973 - acc: 0.9314\n",
      "Epoch 5/10\n",
      "22400/22400 [==============================] - 2s 94us/step - loss: 1.0965 - acc: 0.9314\n",
      "Epoch 6/10\n",
      "22400/22400 [==============================] - 3s 112us/step - loss: 1.0994 - acc: 0.9311\n",
      "Epoch 7/10\n",
      "22400/22400 [==============================] - 2s 105us/step - loss: 1.0954 - acc: 0.9314\n",
      "Epoch 8/10\n",
      "22400/22400 [==============================] - 2s 101us/step - loss: 1.0904 - acc: 0.9312\n",
      "Epoch 9/10\n",
      "22400/22400 [==============================] - 2s 101us/step - loss: 1.0939 - acc: 0.9310\n",
      "Epoch 10/10\n",
      "22400/22400 [==============================] - 2s 96us/step - loss: 0.7660 - acc: 0.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-14 12:29:57,093 : INFO : END GRID SEARCH\n",
      "2018-10-14 12:29:57,094 : INFO : END OF GRID SEARCH\n",
      "2018-10-14 12:29:57,095 : INFO : PRINTING RESULTS\n",
      "2018-10-14 12:29:57,096 : INFO : Best: -0.073637 using {'batch_size': 10, 'epochs': 10, 'optimizer': 'SGD'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean_fit_time', 'mean_score_time', 'mean_test_RMS', 'mean_train_RMS', 'param_batch_size', 'param_epochs', 'param_optimizer', 'params', 'rank_test_RMS', 'split0_test_RMS', 'split0_train_RMS', 'split1_test_RMS', 'split1_train_RMS', 'split2_test_RMS', 'split2_train_RMS', 'std_fit_time', 'std_score_time', 'std_test_RMS', 'std_train_RMS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-14 12:29:57,146 : INFO : SAVED BEST MODEL IN best_model_grid_NN.h5\n",
      "2018-10-14 12:29:57,147 : INFO : EXECUTED IN 115.974321 SEC\n",
      "2018-10-14 12:29:57,149 : INFO : END\n"
     ]
    }
   ],
   "source": [
    "#general\n",
    "import os, traceback\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "from pprint import pprint as pp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "#Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import advanced_activations\n",
    "from keras.models import model_from_json\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#SKLearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from sklearn.cross_validation import PredefinedSplit\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error,r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('')\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "DEF_MODEL_NAME='best_model_grid'\n",
    "CAT_LIST=['categ']\n",
    "#Prameters for RFC\n",
    "param_grid_RFC = dict(n_estimators=[20,50,100,200,],max_features=['auto',2,3,'sqrt'],max_depth=[5,10,15,20,None],min_samples_leaf=[1,10,25,50])\n",
    "#Parameters for NN\n",
    "batch_size = [10, 20]\n",
    "epochs = [10, 15]\n",
    "#activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "activation = ['softmax','relu']\n",
    "optimizer = ['SGD']#, 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid_NN = dict(batch_size=batch_size, epochs=epochs,optimizer=optimizer)\n",
    "# param_grid_NN = dict(activation=['relu','tanh'])\n",
    "PARAM_DICT={'RFC':param_grid_RFC,'NN':param_grid_NN}\n",
    "#Files\n",
    "trainFile = 'promoted.csv'\n",
    "predFile = 'target.csv'\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self,**kargs):\n",
    "        self.path = 'data'\n",
    "        self.trainFile = kargs['tr']\n",
    "        \n",
    "    def readFiles(self,fileName):\n",
    "        fullPath = os.path.join(self.path,fileName)\n",
    "        logger.info('READING %s',fullPath)\n",
    "        df = pd.read_csv(fullPath,sep=',',dtype={'avg_bal':'category', 'geo_group':'category', 'res_type':'category',})#dtype={'avg_bal':'category', 'geo_group':'category', 'res_type':'category',}\n",
    "        logger.info('LOADED DATASET WITH SHAPE %s AND COLUMUNS %s',str(df.shape),str(df.columns))\n",
    "        print('After reading',df.describe())\n",
    "        return df\n",
    "        \n",
    "    def prepareTrain(self):\n",
    "        dfTrain = self.readFiles(self.trainFile)\n",
    "        logger.info('REMOVING ROWS WITH NA')\n",
    "        logger.info('NROWS BEFORE REMOVING NA %i',dfTrain.shape[0])\n",
    "        dfTrain.dropna(inplace=True)\n",
    "        logger.info('NROWS AFTER REMOVING NA %i',dfTrain.shape[0])\n",
    "        X_train = dfTrain.drop(columns=['resp','customer_id'])\n",
    "        Y_train = dfTrain.loc[:,'resp']\n",
    "        logger.info('SCALING OF NUMERIC COLUMNS')\n",
    "        mmscaler = preprocessing.MinMaxScaler()\n",
    "        X_train[['card_tenure', 'risk_score', 'num_promoted']] = mmscaler.fit_transform(X_train[['card_tenure', 'risk_score', 'num_promoted']])  \n",
    "        logger.info('CONSIDERING LEVELS FOR CATEGORICAL COLUMNS')\n",
    "        for curCol in ['avg_bal','geo_group', 'res_type']:\n",
    "            X_train[curCol] = dfTrain[curCol].cat.codes\n",
    "        logger.info('AFTER PREPROCESSING X_train HAS COLUMUNS %s AND TYPES %s',str(X_train.columns),str(X_train.dtypes))\n",
    "        return X_train, Y_train\n",
    "\n",
    "\n",
    "class TestClass:\n",
    "\n",
    "    def __init__(self, **kargs):\n",
    "        # Network Parameters\n",
    "        self.n_hidden_1 = 25 # 1st layer number of neurons\n",
    "        #self.n_hidden_2 = 10 # 2nd layer number of neurons\n",
    "        self.num_input = 6 # MNIST data input (img shape: 28*28)\n",
    "        self.num_classes = 1 \n",
    "        self.activationFun = 'relu'\n",
    "\n",
    "    def createModelNN(self,optimizer):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.num_input, input_dim=self.num_input))\n",
    "        model.add(Dense(self.n_hidden_1 ))\n",
    "        model.add(Dense(self.num_classes,activation='sigmoid'))\n",
    "        # sgd = SGD(lr=model_param['lr'], momentum=model_param['momentum'], decay=0.0, nesterov=False)\n",
    "        #sgd=\"SGD\"\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.fit(X, Y, nb_epoch=numEpoch, batch_size=10)\n",
    "        return model\n",
    "\n",
    "    def createModelRFC(self):\n",
    "        logger.info(\"DEFINITION OF THE MODEL RFC\")\n",
    "        # model = ExtraTreesClassifier(n_estimators=self.model_param['n_estimators'],max_features=self.model_param['max_features'],n_jobs=-1)\n",
    "        # model = RandomForestClassifier(n_jobs=-1),\n",
    "        model = RandomForestRegressor(n_jobs=-1)\n",
    "        logger.info(\"MODEL PARAMS: %s\",model.get_params(deep=True))\n",
    "        return model\n",
    "\n",
    "\n",
    "def gridSearch(model,param_grid,X,Y,scoring):\n",
    "    logger.info(\"START GRID SEARCH\")\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1,scoring=scoring,refit='RMS') \n",
    "    grid_result = grid.fit(X,Y)\n",
    "    logger.info(\"END GRID SEARCH\")\n",
    "    return grid_result\n",
    "\n",
    "def gridResults(grid_result,X,nameModel):\n",
    "    logger.info(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    print(sorted(grid_result.cv_results_.keys()))\n",
    "    if nameModel == \"RFC\":\n",
    "        importances = grid_result.best_estimator_.feature_importances_\n",
    "    if nameModel != 'NN':\n",
    "        df_imp = pd.DataFrame({'features': X.columns.tolist(),'importances':importances})\n",
    "        df_imp.sort_values(by=\"importances\",ascending=False,inplace=True)\n",
    "        print(df_imp)\n",
    "\n",
    "def SaveModel(nameModel,grid_result):\n",
    "    if nameModel=='NN':\n",
    "        outFile='%s_%s.%s'%(DEF_MODEL_NAME,nameModel,\"h5\")\n",
    "    else:\n",
    "        outFile='%s_%s.%s'%(DEF_MODEL_NAME,nameModel,\"pkl\")\n",
    "    joblib.dump(grid_result.best_estimator_,outFile)\n",
    "    logger.info(\"SAVED BEST MODEL IN %s\",outFile)\n",
    "\n",
    "def main(nameModel):\n",
    "    start_time = datetime.now()\n",
    "    logger.info(\"START\")\n",
    "    logger.info('READING THE HYPER PARAMETERS FOR THE SELECTED MODEL')\n",
    "    param_grid = PARAM_DICT[nameModel]\n",
    "    # scoring = {'Accuracy': make_scorer(accuracy_score),'RMS':make_scorer(mean_squared_error)}\n",
    "    logger.info('DEFINING THE SCORING FUNCTION FOR THE GRID SEARCH')\n",
    "    scoring = {'RMS':make_scorer(r2_score)}\n",
    "    logger.info(\"LOADING THE DATA SET\")\n",
    "    ld = LoadData(tr=trainFile)\n",
    "    df2Pred = ld.readFiles(predFile)\n",
    "    X, Y = ld.prepareTrain()\n",
    "    logger.info(\"CREATION OF THE MODEL\")\n",
    "    t=TestClass()\n",
    "    if nameModel == 'NN':\n",
    "        model = KerasClassifier(build_fn=t.createModelNN)\n",
    "#         X = X.as_matrix()\n",
    "#         Y = Y.as_matrix()\n",
    "    else:\n",
    "        model = t.createModelRFC()\n",
    "    logger.info(\"START GRID SEARCH\")\n",
    "    grid_result = gridSearch(model,param_grid,X,Y,scoring)\n",
    "    logger.info(\"END OF GRID SEARCH\")\n",
    "    logger.info(\"PRINTING RESULTS\")\n",
    "    gridResults(grid_result,X,nameModel)\n",
    "    SaveModel(nameModel,grid_result)\n",
    "    logger.info(\"EXECUTED IN %f SEC\"%((datetime.now()-start_time)).total_seconds())\n",
    "    logger.info(\"END\")\n",
    "    return grid_result\n",
    "grid_result = main('NN')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
