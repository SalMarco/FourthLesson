{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Introduction to Python\n",
    "================================\n",
    "\n",
    "Lesson 3 - Part1\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "\n",
    "In this lesson we will explore a little more in dept some aspects of Keras and neural networks.\n",
    "The topis that we'll cover in this lesson are:\n",
    "\n",
    "  - Dropout\n",
    "  - Grid Search\n",
    "  - Autoencoders\n",
    "  \n",
    "But before starting with the real Lesson let's make a little bit of informations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Informations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exam modality \n",
    "\n",
    "The exams will be a code snippet to comment.\n",
    "\n",
    "It may be required to give an high level or a *line by line* explanation.\n",
    "\n",
    "The arguments will be the same that we have touched in this four lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keep in touch #1\n",
    "\n",
    "If some of you are interested in a stage/work as data scientist in Leroy Merlin please contact me at:\n",
    "\n",
    "  - marco.saletta@leroymerlin.it\n",
    "\n",
    "We are organizing a data science unit and maybe we can work together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keep in touch #2\n",
    "\n",
    "The datascience environment (and the media/digital in general) in Milan is very small place and contacts are very important.\n",
    "\n",
    "If you are looking for a looking for a job, and you are not interested in Leroy Merlin, maybe let me know at\n",
    "\n",
    "  - marco.saletta@gmail.com\n",
    "\n",
    "Often someone asks me if I know a datascientis who is looking for a job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keep in touch #3\n",
    "\n",
    "In Milan there are many events related to datascience that are good for keeping up to date and being involved in the community.\n",
    "\n",
    "The two I want to address to you are:\n",
    "\n",
    "  - [Data Beers Milan](https://www.meetup.com/it-IT/DataBeers-Milano/?_cookie-check=ma6kEV7Q72i7FAiv): free beers and funny datascience talks\n",
    "  - [Data Science Milan](http://datasciencemilan.org/): nice talks with industrial sponsors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with the lesson\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Droptout\n",
    "\n",
    "Dropout is a method often used on neural networks in order to avoid/reduce orverfitting.\n",
    "\n",
    "The idea is to randomly shut down some neurons at each iteration in order to make them less relevant for the network.   \n",
    "\n",
    "For this dropout is a layer itself, as you can see from the [Keras docs](https://keras.io/layers/core/), that we put on top of another layer.\n",
    "\n",
    "Let's see how we can improve the network that we have created for the **mnist** classification task last lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Re-factoring\n",
    "\n",
    "In order to improve the code even more, we'll make a little bit of re-factoring and re-write it using Python Classes.\n",
    "\n",
    "**NOTE**: please note how the names of classes and function are written. As you can see \n",
    "\n",
    "  - every class has a name that starts with a **uppercase** letter\n",
    "  - every function has a name that starts with a **lowercase** letter\n",
    "  \n",
    "This is due to a style convention used in Python coding called [pep8](https://www.python.org/dev/peps/pep-0008/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise \n",
    "\n",
    "For the code below:\n",
    "\n",
    "  - Give an *high level* description of what the code does\n",
    "  - Give a *line by line* description of the methods `modelDefinition` and the class `PlotGraphs`.\n",
    "\n",
    "**HINT**: `string.find(substring)` searches for a substring in a string. Returns `True` if present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import logging\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import advanced_activations\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('')\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 15\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "metrics = ['accuracy','mae']\n",
    "\n",
    "\n",
    "class PrepareVariabiles:\n",
    "    def __init__(self):\n",
    "        self.mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "        \n",
    "        \n",
    "    def splitDatset(self):\n",
    "        X = self.mnist.train.images\n",
    "        Y = self.mnist.train.labels\n",
    "        X_test = self.mnist.test.images\n",
    "        Y_test = self.mnist.test.labels\n",
    "        return X,Y,X_test,Y_test\n",
    "    \n",
    "class CreateNN:\n",
    "    def __init__(self,**kargs):\n",
    "        self.activationFun = 'relu'\n",
    "        self.dropPerc = 0.25\n",
    "        self.X = kargs['x']\n",
    "        self.Y = kargs['y']\n",
    "        self.X_test = kargs['xt']\n",
    "        self.Y_test = kargs['yt']\n",
    "        \n",
    "        \n",
    "    def modelDefinition(self,useDropout=False):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(num_input, input_dim=num_input,activation=self.activationFun))\n",
    "        if useDropout:\n",
    "            self.model.add(Dropout(self.dropPerc))\n",
    "        self.model.add(Dense(n_hidden_1,activation=self.activationFun))\n",
    "        if useDropout:\n",
    "            self.model.add(Dropout(self.dropPerc))\n",
    "        self.model.add(Dense(n_hidden_2,activation=self.activationFun))\n",
    "        if useDropout:\n",
    "            self.model.add(Dropout(self.dropPerc))\n",
    "        self.model.add(Dense(num_classes,activation='softmax'))\n",
    "        \n",
    "    def modelCompile(self):\n",
    "        adam = Adam(lr=learning_rate)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=adam,metrics=metrics)\n",
    "        \n",
    "    def modelEval(self):\n",
    "        tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph',write_graph=True, write_images=True,embeddings_freq=0)\n",
    "        history = self.model.fit(self.X, self.Y, epochs=num_steps, batch_size=batch_size,validation_data=(self.X_test,self.Y_test),callbacks=[tbCallBack])#validation_split=0.05\n",
    "        scores = self.model.evaluate(self.X_test, self.Y_test)\n",
    "        return history,scores,self.model\n",
    "    \n",
    "class PlotGraphs:\n",
    "    def __init__(self,**kargs):\n",
    "        history= kargs['h']\n",
    "        self.history_dict = history.history\n",
    "        self.metList = []\n",
    "        for cur_key in history.history.keys():\n",
    "            if cur_key.find('val')!=0:\n",
    "                self.metList.append(cur_key)\n",
    "        print(self.metList)\n",
    "        self.lenList = len(self.metList)\n",
    "    \n",
    "    def plotResults(self):\n",
    "        plotPos = 1\n",
    "        for cur_met in self.metList:\n",
    "            cur_values = self.history_dict[cur_met]\n",
    "            cur_val = self.history_dict['val_%s'%cur_met]\n",
    "            epochs = range(1, len(cur_values) + 1)\n",
    "            plt.subplot(self.lenList, 1, plotPos)\n",
    "            plt.plot(epochs, cur_values, 'ro')\n",
    "            plt.plot(epochs, cur_val, 'b+')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel(cur_met)\n",
    "            plotPos += 1\n",
    "\n",
    "\n",
    "def main():\n",
    "    start_time = datetime.now()\n",
    "    logger.info('START')\n",
    "    logger.info('INITIALIZATION OF PREPAREVARIABILES')\n",
    "    pv = PrepareVariabiles()\n",
    "    X, Y, X_test, Y_test = pv.splitDatset()\n",
    "    logger.info('INITIALIZATION OF CREATENN')\n",
    "    cnn = CreateNN(x=X,y=Y,xt=X_test,yt=Y_test)\n",
    "    cnn.modelDefinition()#useDropout=True\n",
    "    cnn.modelCompile()\n",
    "    history, scores, model = cnn.modelEval()\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    logger.info('INITIALIZATION OF PLOTGRAPH')\n",
    "    pg = PlotGraphs(h=history)\n",
    "    pg.plotResults()\n",
    "    logger.info(\"EXECUTED IN %f SEC\"%((datetime.now()-start_time)).total_seconds())\n",
    "    logger.info('END')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "history = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "history.history.keys()\n",
    "# for cur_key in history.history.keys():\n",
    "#     if cur_key.find('val')!=0:\n",
    "#         print(cur_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensorboard\n",
    "\n",
    "Tensorboard is a tool for make a visual inspection on a netwoek created with Tensorflow (ans so also with Keras).\n",
    "\n",
    "The argument is very complex and you can find more information at [Keras callbacks man page](https://keras.io/callbacks/). We won't go in any details now.\n",
    "\n",
    "The command that enables us to use Tendorboard is\n",
    "\n",
    "`tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph',write_graph=True, write_images=True,embeddings_freq=0)`\n",
    "\n",
    "The callback is used in fit by `callbacks=[tbCallBack]` ad produces a series of file in the `log_dir` folder (it also create it).\n",
    "\n",
    "The result is visible running `tensorboard --logdir Graph` and opening at `http://localhost:6006` in the browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid Search \n",
    "\n",
    "The `grid search` is the names the `sklearn` uses to indicate the **hyper parameter optimization** ([form wiki](https://en.wikipedia.org/wiki/Hyperparameter_optimization)):\n",
    "\n",
    "*In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.\n",
    "The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance.*\n",
    "\n",
    "The package that we'll use is `GridSearchCV` from `sklearn.model_selection` ([documentation page](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).\n",
    "\n",
    "As you can see at main Sklearn's page about [Tuning Hyper Parameters](http://scikit-learn.org/stable/modules/grid_search.html), this is just one of the possibility that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid Search\n",
    "\n",
    "In order to make highlight some implementation's difference and make the code more general, we'll see how to use the grid search over two different kind of models:\n",
    "\n",
    "  - NN (`keras`)\n",
    "  - Random Forest (`sklearn`)\n",
    "  \n",
    "**NOTE**: Since all `sklearn` models have the same structure, the same code can be used for other algorithms.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise \n",
    "\n",
    "For the code below:\n",
    "\n",
    "  - Give a *line by line* description of the `LoadData` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#general\n",
    "import os, traceback\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "from pprint import pprint as pp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "#Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import advanced_activations\n",
    "from keras.models import model_from_json\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#SKLearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error,r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('')\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "DEF_MODEL_NAME='best_model_grid'\n",
    "CAT_LIST=['categ']\n",
    "#Prameters for RFC\n",
    "param_grid_RFC = dict(n_estimators=[20,50],max_features=['auto',2],max_depth=[5,10,None],min_samples_leaf=[1,10])\n",
    "#Parameters for NN\n",
    "batch_size = [5,10]\n",
    "epochs = [5,10]\n",
    "#activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "activation = ['softmax','relu']\n",
    "optimizer = ['SGD']#, 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid_NN = dict(batch_size=batch_size, epochs=epochs,optimizer=optimizer)\n",
    "# param_grid_NN = dict(activation=['relu','tanh'])\n",
    "PARAM_DICT={'RFC':param_grid_RFC,'NN':param_grid_NN}\n",
    "#Files\n",
    "trainFile = 'promoted.csv'\n",
    "predFile = 'target.csv'\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self,**kargs):\n",
    "        self.path = 'data'\n",
    "        self.trainFile = kargs['tr']\n",
    "        \n",
    "    def readFiles(self,fileName):\n",
    "        fullPath = os.path.join(self.path,fileName)\n",
    "        logger.info('READING %s',fullPath)\n",
    "        df = pd.read_csv(fullPath,sep=',',dtype={'avg_bal':'category', 'geo_group':'category', 'res_type':'category',})#dtype={'avg_bal':'category', 'geo_group':'category', 'res_type':'category',}\n",
    "        logger.info('LOADED DATASET WITH SHAPE %s AND COLUMUNS %s',str(df.shape),str(df.columns))\n",
    "        print('After reading',df.describe())\n",
    "        return df\n",
    "        \n",
    "    def prepareTrain(self):\n",
    "        dfTrain = self.readFiles(self.trainFile)\n",
    "        dfTrain.dropna(inplace=True)\n",
    "        X_train = dfTrain.drop(columns=['resp','customer_id'])\n",
    "        Y_train = dfTrain.loc[:,'resp']\n",
    "        mmscaler = preprocessing.MinMaxScaler()\n",
    "        X_train[['card_tenure', 'risk_score', 'num_promoted']] = mmscaler.fit_transform(X_train[['card_tenure', 'risk_score', 'num_promoted']])  \n",
    "        for curCol in ['avg_bal','geo_group', 'res_type']:\n",
    "            X_train[curCol] = dfTrain[curCol].cat.codes\n",
    "        logger.info('AFTER PREPROCESSING X_train HAS COLUMUNS %s AND TYPES %s',str(X_train.columns),str(X_train.dtypes))\n",
    "        return X_train, Y_train\n",
    "\n",
    "\n",
    "class TestClass:\n",
    "\n",
    "    def __init__(self, **kargs):\n",
    "        # Network Parameters\n",
    "        self.n_hidden_1 = 25 \n",
    "        #self.n_hidden_2 = 10 \n",
    "        self.num_input = 6 \n",
    "        \n",
    "        self.num_classes = 1 \n",
    "        self.activationFun = 'relu'\n",
    "\n",
    "    def createModelNN(self,optimizer):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.num_input, input_dim=self.num_input))\n",
    "        model.add(Dense(self.n_hidden_1 ))\n",
    "        model.add(Dense(self.num_classes,activation='sigmoid'))\n",
    "        # sgd = SGD(lr=model_param['lr'], momentum=model_param['momentum'], decay=0.0, nesterov=False)\n",
    "        #sgd=\"SGD\"\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def createModelRFC(self):\n",
    "        logger.info(\"DEFINITION OF THE MODEL RFC\")\n",
    "        model = RandomForestClassifier(n_jobs=-1)\n",
    "        logger.info(\"MODEL PARAMS: %s\",model.get_params(deep=True))\n",
    "        return model\n",
    "\n",
    "\n",
    "def gridSearch(model,param_grid,X,Y,scoring):\n",
    "    logger.info(\"START GRID SEARCH\")\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1,scoring=scoring,refit='RMS',verbose=3) \n",
    "    grid_result = grid.fit(X,Y)\n",
    "    logger.info(\"END GRID SEARCH\")\n",
    "    return grid_result\n",
    "\n",
    "def gridResults(grid_result,X,nameModel):\n",
    "    logger.info(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    print(sorted(grid_result.cv_results_.keys()))\n",
    "    if nameModel == \"RFC\":\n",
    "        importances = grid_result.best_estimator_.feature_importances_\n",
    "    if nameModel != 'NN':\n",
    "        df_imp = pd.DataFrame({'features': X.columns.tolist(),'importances':importances})\n",
    "        df_imp.sort_values(by=\"importances\",ascending=False,inplace=True)\n",
    "        print(df_imp)\n",
    "\n",
    "def SaveModel(nameModel,grid_result):\n",
    "    if nameModel=='NN':\n",
    "        outFile='%s_%s.%s'%(DEF_MODEL_NAME,nameModel,\"h5\")\n",
    "    else:\n",
    "        outFile='%s_%s.%s'%(DEF_MODEL_NAME,nameModel,\"pkl\")\n",
    "    joblib.dump(grid_result.best_estimator_,outFile)\n",
    "    logger.info(\"SAVED BEST MODEL IN %s\",outFile)\n",
    "\n",
    "def main(nameModel):\n",
    "    start_time = datetime.now()\n",
    "    logger.info(\"START\")\n",
    "    logger.info('READING THE HYPER PARAMETERS FOR THE SELECTED MODEL')\n",
    "    param_grid = PARAM_DICT[nameModel]\n",
    "    logger.info('DEFINING THE SCORING FUNCTION FOR THE GRID SEARCH')\n",
    "    scoring = {'RMS':make_scorer(r2_score)}\n",
    "    # scoring = {'Accuracy': make_scorer(accuracy_score),'RMS':make_scorer(mean_squared_error)}\n",
    "    logger.info(\"LOADING THE DATA SET\")\n",
    "    ld = LoadData(tr=trainFile)\n",
    "    df2Pred = ld.readFiles(predFile)\n",
    "    X, Y = ld.prepareTrain()\n",
    "    logger.info(\"CREATION OF THE MODEL\")\n",
    "    t=TestClass()\n",
    "    if nameModel == 'NN':\n",
    "        model = KerasClassifier(build_fn=t.createModelNN)\n",
    "    else:\n",
    "        model = t.createModelRFC()\n",
    "    logger.info(\"START GRID SEARCH\")\n",
    "    grid_result = gridSearch(model,param_grid,X,Y,scoring)\n",
    "    logger.info(\"END OF GRID SEARCH\")\n",
    "    logger.info(\"PRINTING RESULTS\")\n",
    "    gridResults(grid_result,X,nameModel)\n",
    "    SaveModel(nameModel,grid_result)\n",
    "    logger.info(\"EXECUTED IN %f SEC\"%((datetime.now()-start_time)).total_seconds())\n",
    "    logger.info(\"END\")\n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-21 19:34:06,712 : INFO : START\n",
      "2018-10-21 19:34:06,714 : INFO : READING THE HYPER PARAMETERS FOR THE SELECTED MODEL\n",
      "2018-10-21 19:34:06,714 : INFO : DEFINING THE SCORING FUNCTION FOR THE GRID SEARCH\n",
      "2018-10-21 19:34:06,716 : INFO : LOADING THE DATA SET\n",
      "2018-10-21 19:34:06,717 : INFO : READING data/target.csv\n",
      "2018-10-21 19:34:06,874 : INFO : LOADED DATASET WITH SHAPE (110000, 8) AND COLUMUNS Index(['customer_id', 'card_tenure', 'risk_score', 'num_promoted', 'avg_bal',\n",
      "       'geo_group', 'res_type', 'Unnamed: 7'],\n",
      "      dtype='object')\n",
      "2018-10-21 19:34:06,917 : INFO : READING data/promoted.csv\n",
      "2018-10-21 19:34:06,960 : INFO : LOADED DATASET WITH SHAPE (25000, 8) AND COLUMUNS Index(['customer_id', 'resp', 'card_tenure', 'risk_score', 'num_promoted',\n",
      "       'avg_bal', 'geo_group', 'res_type'],\n",
      "      dtype='object')\n",
      "2018-10-21 19:34:07,011 : INFO : AFTER PREPROCESSING X_train HAS COLUMUNS Index(['card_tenure', 'risk_score', 'num_promoted', 'avg_bal', 'geo_group',\n",
      "       'res_type'],\n",
      "      dtype='object') AND TYPES card_tenure     float64\n",
      "risk_score      float64\n",
      "num_promoted    float64\n",
      "avg_bal           int16\n",
      "geo_group          int8\n",
      "res_type           int8\n",
      "dtype: object\n",
      "2018-10-21 19:34:07,013 : INFO : CREATION OF THE MODEL\n",
      "2018-10-21 19:34:07,013 : INFO : START GRID SEARCH\n",
      "2018-10-21 19:34:07,014 : INFO : START GRID SEARCH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reading          card_tenure     risk_score   num_promoted  Unnamed: 7\n",
      "count  107792.000000  110000.000000  110000.000000         0.0\n",
      "mean      138.956564     655.571482       0.006782         NaN\n",
      "std        67.433081      81.252328       0.082183         NaN\n",
      "min        12.000000     520.000000       0.000000         NaN\n",
      "25%        91.000000     600.000000       0.000000         NaN\n",
      "50%       135.000000     678.000000       0.000000         NaN\n",
      "75%       179.000000     720.000000       0.000000         NaN\n",
      "max       641.000000     760.000000       2.000000         NaN\n",
      "After reading                resp   card_tenure    risk_score  num_promoted\n",
      "count  25000.000000  24515.000000  25000.000000  25000.000000\n",
      "mean       0.068640    139.491617    655.091680      0.007000\n",
      "std        0.252846     66.998010     81.315116      0.083374\n",
      "min        0.000000      0.000000    520.000000      0.000000\n",
      "25%        0.000000     95.000000    599.000000      0.000000\n",
      "50%        0.000000    135.000000    677.000000      0.000000\n",
      "75%        0.000000    179.000000    719.000000      0.000000\n",
      "max        1.000000    641.000000    760.000000      1.000000\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] batch_size=5, epochs=5, optimizer=SGD ...........................\n",
      "[CV] batch_size=5, epochs=5, optimizer=SGD ...........................\n",
      "[CV] batch_size=5, epochs=5, optimizer=SGD ...........................\n",
      "[CV] batch_size=5, epochs=10, optimizer=SGD ..........................\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "Epoch 1/10\n",
      "14934/14934 [==============================] - 6s 375us/step - loss: 3.6701 - acc: 0.7688\n",
      "Epoch 2/5\n",
      "14933/14933 [==============================] - 6s 378us/step - loss: 1.3013 - acc: 0.9180\n",
      "Epoch 2/5\n",
      "14933/14933 [==============================] - 6s 374us/step - loss: 1.3006 - acc: 0.9180\n",
      "Epoch 2/10\n",
      "14933/14933 [==============================] - 6s 386us/step - loss: 3.2318 - acc: 0.7964\n",
      "Epoch 2/5\n",
      "14934/14934 [==============================] - 5s 349us/step - loss: 1.7792 - acc: 0.8880\n",
      "Epoch 3/5\n",
      "14933/14933 [==============================] - 5s 351us/step - loss: 1.0727 - acc: 0.9325\n",
      "Epoch 3/10\n",
      "14933/14933 [==============================] - 5s 355us/step - loss: 1.0739 - acc: 0.9328\n",
      "Epoch 3/5\n",
      "14933/14933 [==============================] - 5s 353us/step - loss: 1.1428 - acc: 0.9287\n",
      "Epoch 3/5\n",
      "14933/14933 [==============================] - 6s 376us/step - loss: 1.0745 - acc: 0.9328\n",
      "14365/14933 [===========================>..] - ETA: 0s - loss: 1.1371 - acc: 0.9288Epoch 4/10\n",
      "14934/14934 [==============================] - 6s 386us/step - loss: 1.0761 - acc: 0.9322\n",
      "Epoch 4/5\n",
      "14933/14933 [==============================] - 6s 383us/step - loss: 4.3869 - acc: 0.7239\n",
      "Epoch 4/5\n",
      "14933/14933 [==============================] - 6s 381us/step - loss: 1.1425 - acc: 0.9285\n",
      "Epoch 4/5\n",
      "14933/14933 [==============================] - 7s 444us/step - loss: 1.0733 - acc: 0.9328\n",
      "Epoch 5/10\n",
      "14934/14934 [==============================] - 7s 454us/step - loss: 1.1520 - acc: 0.9273\n",
      "Epoch 5/5\n",
      "14933/14933 [==============================] - 7s 450us/step - loss: 1.0734 - acc: 0.9328\n",
      "Epoch 5/5\n",
      "14933/14933 [==============================] - 7s 448us/step - loss: 1.1423 - acc: 0.9287\n",
      "Epoch 5/5\n",
      "14933/14933 [==============================] - 6s 375us/step - loss: 1.0732 - acc: 0.9326\n",
      "Epoch 6/10\n",
      "14934/14934 [==============================] - 5s 368us/step - loss: 1.0731 - acc: 0.9327\n",
      "14933/14933 [==============================] - 6s 373us/step - loss: 1.0714 - acc: 0.9328\n",
      "14933/14933 [==============================] - 6s 372us/step - loss: 1.1415 - acc: 0.9285\n",
      " 7640/14933 [==============>...............] - ETA: 2s - loss: 1.0993 - acc: 0.9312[CV]  batch_size=5, epochs=5, optimizer=SGD, RMS=-0.0765681326604184, total=  30.4s\n",
      "[CV] batch_size=5, epochs=10, optimizer=SGD ..........................\n",
      " 7845/14933 [==============>...............] - ETA: 1s - loss: 1.1014 - acc: 0.9310[CV]  batch_size=5, epochs=5, optimizer=SGD, RMS=-0.07686760888376143, total=  30.5s\n",
      "[CV] batch_size=5, epochs=10, optimizer=SGD ..........................\n",
      " 7935/14933 [==============>...............] - ETA: 1s - loss: 1.1011 - acc: 0.9311[CV]  batch_size=5, epochs=5, optimizer=SGD, RMS=-0.0674767691208007, total=  30.5s\n",
      "[CV] batch_size=10, epochs=5, optimizer=SGD ..........................\n",
      "12725/14933 [========================>.....] - ETA: 0s - loss: 1.0630 - acc: 0.9334Epoch 1/10\n",
      "12905/14933 [========================>.....] - ETA: 0s - loss: 1.0694 - acc: 0.9330Epoch 1/10\n",
      "Epoch 1/5\n",
      "14933/14933 [==============================] - 4s 279us/step - loss: 1.0732 - acc: 0.9328\n",
      "Epoch 7/10\n",
      "14933/14933 [==============================] - 4s 252us/step - loss: 1.0758 - acc: 0.9327\n",
      "Epoch 2/5\n",
      "14933/14933 [==============================] - 5s 361us/step - loss: 1.0735 - acc: 0.9325\n",
      "Epoch 8/10\n",
      "14933/14933 [==============================] - 6s 426us/step - loss: 1.1434 - acc: 0.9287\n",
      "Epoch 2/10\n",
      "14934/14934 [==============================] - 6s 430us/step - loss: 5.4528 - acc: 0.6569\n",
      "Epoch 2/10\n",
      "14933/14933 [==============================] - 3s 184us/step - loss: 1.0838 - acc: 0.9322\n",
      "Epoch 3/5\n",
      "14933/14933 [==============================] - 3s 175us/step - loss: 1.0752 - acc: 0.9328\n",
      "Epoch 4/5\n",
      "14933/14933 [==============================] - 5s 350us/step - loss: 1.0706 - acc: 0.9322\n",
      "Epoch 9/10\n",
      "14933/14933 [==============================] - 6s 386us/step - loss: 1.1436 - acc: 0.9287\n",
      "Epoch 3/10\n",
      "14934/14934 [==============================] - 6s 384us/step - loss: 8.5287 - acc: 0.4641\n",
      "Epoch 3/10\n",
      "14933/14933 [==============================] - 3s 204us/step - loss: 1.0751 - acc: 0.9328\n",
      "Epoch 5/5\n",
      "14933/14933 [==============================] - 3s 221us/step - loss: 1.0772 - acc: 0.9325\n",
      "14933/14933 [==============================] - 6s 403us/step - loss: 1.0538 - acc: 0.9314\n",
      "Epoch 10/10\n",
      "12530/14933 [========================>.....] - ETA: 0s - loss: 1.1576 - acc: 0.9278[CV]  batch_size=10, epochs=5, optimizer=SGD, RMS=-0.07686760888376143, total=  17.5s\n",
      "[CV] batch_size=10, epochs=5, optimizer=SGD ..........................\n",
      "14933/14933 [==============================] - 6s 413us/step - loss: 1.1483 - acc: 0.9284\n",
      "Epoch 4/10\n",
      "14934/14934 [==============================] - 6s 413us/step - loss: 1.0782 - acc: 0.9327\n",
      "Epoch 4/10\n",
      " 5100/14933 [=========>....................] - ETA: 3s - loss: 1.0768 - acc: 0.9278Epoch 1/5\n",
      "14933/14933 [==============================] - 6s 392us/step - loss: 0.6425 - acc: 0.9286\n",
      "14933/14933 [==============================] - 5s 321us/step - loss: 1.1441 - acc: 0.9287\n",
      "13495/14933 [==========================>...] - ETA: 0s - loss: 1.1339 - acc: 0.9294Epoch 2/5\n",
      "14934/14934 [==============================] - 6s 412us/step - loss: 3.1052 - acc: 0.8049\n",
      "Epoch 5/10\n",
      "14933/14933 [==============================] - 6s 423us/step - loss: 1.1437 - acc: 0.9287\n",
      "  135/14934 [..............................] - ETA: 6s - loss: 0.8358 - acc: 0.9481    Epoch 5/10\n",
      " 4590/14934 [========>.....................] - ETA: 4s - loss: 1.0053 - acc: 0.9373[CV]  batch_size=5, epochs=10, optimizer=SGD, RMS=-0.07686760888376143, total=  57.5s\n",
      " 4425/14933 [=======>......................] - ETA: 4s - loss: 1.1914 - acc: 0.9261[CV] batch_size=10, epochs=5, optimizer=SGD ..........................\n",
      "14933/14933 [==============================] - 3s 216us/step - loss: 1.1440 - acc: 0.9287\n",
      "Epoch 3/5\n",
      "10345/14933 [===================>..........] - ETA: 1s - loss: 1.1303 - acc: 0.9297Epoch 1/5\n",
      "14933/14933 [==============================] - 3s 180us/step - loss: 1.1439 - acc: 0.9287\n",
      "  210/14934 [..............................] - ETA: 1:19 - loss: 1.4598 - acc: 0.9095     Epoch 4/5\n",
      "14934/14934 [==============================] - 6s 392us/step - loss: 1.0769 - acc: 0.9327\n",
      "Epoch 6/10\n",
      "14933/14933 [==============================] - 6s 391us/step - loss: 1.4768 - acc: 0.9078\n",
      "Epoch 6/10\n",
      "14934/14934 [==============================] - 4s 281us/step - loss: 1.0778 - acc: 0.9327\n",
      "Epoch 2/5\n",
      "14933/14933 [==============================] - 3s 213us/step - loss: 1.1437 - acc: 0.9287\n",
      "Epoch 5/5\n",
      "14934/14934 [==============================] - 3s 219us/step - loss: 1.0776 - acc: 0.9327\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14933/14933 [==============================] - 3s 217us/step - loss: 1.1439 - acc: 0.9287\n",
      "14934/14934 [==============================] - 6s 413us/step - loss: 1.0769 - acc: 0.9324\n",
      "Epoch 7/10\n",
      "14933/14933 [==============================] - 6s 417us/step - loss: 1.1426 - acc: 0.9287\n",
      "Epoch 7/10\n",
      " 4500/14934 [========>.....................] - ETA: 4s - loss: 1.0879 - acc: 0.9316[CV]  batch_size=10, epochs=5, optimizer=SGD, RMS=-0.0674767691208007, total=  19.9s\n",
      "[CV] batch_size=10, epochs=10, optimizer=SGD .........................\n",
      "14934/14934 [==============================] - 3s 208us/step - loss: 1.0776 - acc: 0.9327\n",
      "Epoch 4/5\n",
      " 5960/14934 [==========>...................] - ETA: 1s - loss: 1.0759 - acc: 0.9329Epoch 1/10\n",
      "14934/14934 [==============================] - 3s 188us/step - loss: 1.0775 - acc: 0.9327\n",
      "Epoch 5/5\n",
      "14934/14934 [==============================] - 6s 388us/step - loss: 1.0762 - acc: 0.9327\n",
      "Epoch 8/10\n",
      "14933/14933 [==============================] - 6s 391us/step - loss: 1.1435 - acc: 0.9287\n",
      "Epoch 8/10\n",
      "14934/14934 [==============================] - 3s 200us/step - loss: 1.0775 - acc: 0.9327\n",
      "14933/14933 [==============================] - 5s 315us/step - loss: 1.4847 - acc: 0.9052\n",
      "Epoch 2/10\n",
      "11535/14933 [======================>.......] - ETA: 1s - loss: 1.1423 - acc: 0.9288[CV]  batch_size=10, epochs=5, optimizer=SGD, RMS=-0.0785955679384982, total=  19.2s\n",
      "[CV] batch_size=10, epochs=10, optimizer=SGD .........................\n",
      "14934/14934 [==============================] - 6s 380us/step - loss: 1.0746 - acc: 0.9327\n",
      "Epoch 9/10\n",
      "14933/14933 [==============================] - 6s 380us/step - loss: 1.1452 - acc: 0.9284\n",
      "Epoch 9/10\n",
      "14933/14933 [==============================] - 3s 199us/step - loss: 2.3945 - acc: 0.8497\n",
      "Epoch 3/10\n",
      " 2350/14933 [===>..........................] - ETA: 2s - loss: 14.6277 - acc: 0.0796Epoch 1/10\n",
      "14933/14933 [==============================] - 3s 202us/step - loss: 3.3940 - acc: 0.7866\n",
      "Epoch 4/10\n",
      "14933/14933 [==============================] - 4s 278us/step - loss: 1.5504 - acc: 0.9021\n",
      "Epoch 2/10\n",
      "14934/14934 [==============================] - 6s 371us/step - loss: 1.0763 - acc: 0.9326\n",
      "Epoch 10/10\n",
      "14933/14933 [==============================] - 5s 368us/step - loss: 1.1435 - acc: 0.9287\n",
      "Epoch 10/10\n",
      "14933/14933 [==============================] - 3s 202us/step - loss: 1.0752 - acc: 0.9328\n",
      "Epoch 5/10\n",
      "14933/14933 [==============================] - 3s 196us/step - loss: 1.2823 - acc: 0.9196\n",
      "Epoch 3/10\n",
      "14933/14933 [==============================] - 3s 197us/step - loss: 1.0751 - acc: 0.9328\n",
      "Epoch 6/10\n",
      "14933/14933 [==============================] - 3s 195us/step - loss: 1.1438 - acc: 0.9287\n",
      "Epoch 4/10\n",
      "14934/14934 [==============================] - 6s 382us/step - loss: 1.0768 - acc: 0.9325\n",
      "14933/14933 [==============================] - 6s 378us/step - loss: 1.1435 - acc: 0.9287\n",
      "14933/14933 [==============================] - 3s 195us/step - loss: 1.2743 - acc: 0.9197\n",
      "Epoch 7/10\n",
      "14933/14933 [==============================] - 3s 178us/step - loss: 1.1438 - acc: 0.9287\n",
      "Epoch 5/10\n",
      " 3630/14933 [======>.......................] - ETA: 1s - loss: 1.2392 - acc: 0.9231[CV]  batch_size=5, epochs=10, optimizer=SGD, RMS=-0.0674767691208007, total= 1.0min\n",
      "[CV] batch_size=10, epochs=10, optimizer=SGD .........................\n",
      " 3810/14933 [======>.......................] - ETA: 1s - loss: 1.2145 - acc: 0.9247[CV]  batch_size=5, epochs=10, optimizer=SGD, RMS=-0.0765681326604184, total= 1.0min\n",
      "14933/14933 [==============================] - 3s 173us/step - loss: 1.0754 - acc: 0.9328\n",
      "Epoch 8/10\n",
      "10790/14933 [====================>.........] - ETA: 0s - loss: 1.1292 - acc: 0.9297Epoch 1/10\n",
      "14933/14933 [==============================] - 2s 162us/step - loss: 1.1437 - acc: 0.9287\n",
      "Epoch 6/10\n",
      "14933/14933 [==============================] - 3s 173us/step - loss: 1.0754 - acc: 0.9328\n",
      "Epoch 9/10\n",
      "14933/14933 [==============================] - 3s 178us/step - loss: 7.3249 - acc: 0.5399\n",
      "Epoch 7/10\n",
      "14934/14934 [==============================] - 4s 241us/step - loss: 1.5375 - acc: 0.9026\n",
      "Epoch 2/10\n",
      "14933/14933 [==============================] - 3s 179us/step - loss: 1.0754 - acc: 0.9328\n",
      "Epoch 10/10\n",
      "14933/14933 [==============================] - 3s 176us/step - loss: 5.3224 - acc: 0.6654\n",
      "Epoch 8/10\n",
      "14934/14934 [==============================] - 3s 175us/step - loss: 1.0724 - acc: 0.9327\n",
      "Epoch 3/10\n",
      "14933/14933 [==============================] - 3s 199us/step - loss: 1.0753 - acc: 0.9328\n",
      "14933/14933 [==============================] - 3s 184us/step - loss: 1.1421 - acc: 0.9287\n",
      "Epoch 9/10\n",
      "14934/14934 [==============================] - 3s 179us/step - loss: 1.6709 - acc: 0.8949\n",
      "Epoch 4/10\n",
      " 3150/14934 [=====>........................] - ETA: 1s - loss: 0.9478 - acc: 0.9403[CV]  batch_size=10, epochs=10, optimizer=SGD, RMS=-0.07686760888376143, total=  33.2s\n",
      " 4550/14934 [========>.....................] - ETA: 1s - loss: 1.0326 - acc: 0.9352"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed:  1.8min remaining:   21.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14933/14933 [==============================] - 2s 145us/step - loss: 1.1503 - acc: 0.9281\n",
      "13520/14934 [==========================>...] - ETA: 0s - loss: 1.0747 - acc: 0.9326Epoch 10/10\n",
      "14934/14934 [==============================] - 2s 144us/step - loss: 1.0745 - acc: 0.9327\n",
      "Epoch 5/10\n",
      "14933/14933 [==============================] - 2s 160us/step - loss: 1.1436 - acc: 0.9287\n",
      "14934/14934 [==============================] - 2s 156us/step - loss: 1.0769 - acc: 0.9327\n",
      "Epoch 6/10\n",
      " 6900/14934 [============>.................] - ETA: 1s - loss: 1.0533 - acc: 0.9343[CV]  batch_size=10, epochs=10, optimizer=SGD, RMS=-0.0674767691208007, total=  30.0s\n",
      "14934/14934 [==============================] - 2s 114us/step - loss: 1.0768 - acc: 0.9327\n",
      "Epoch 7/10\n",
      "14934/14934 [==============================] - 1s 96us/step - loss: 1.0758 - acc: 0.9322\n",
      "Epoch 8/10\n",
      "14934/14934 [==============================] - 2s 103us/step - loss: 1.0773 - acc: 0.9327\n",
      "Epoch 9/10\n",
      "14934/14934 [==============================] - 1s 99us/step - loss: 1.0773 - acc: 0.9327\n",
      "Epoch 10/10\n",
      "14934/14934 [==============================] - 2s 109us/step - loss: 1.0772 - acc: 0.9327\n",
      "[CV]  batch_size=10, epochs=10, optimizer=SGD, RMS=-0.0765681326604184, total=  22.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "22400/22400 [==============================] - 4s 190us/step - loss: 4.5109 - acc: 0.7160\n",
      "Epoch 2/5\n",
      "22400/22400 [==============================] - 4s 167us/step - loss: 1.0973 - acc: 0.9314\n",
      "Epoch 3/5\n",
      "22400/22400 [==============================] - 4s 199us/step - loss: 1.1082 - acc: 0.9304\n",
      "Epoch 4/5\n",
      "22400/22400 [==============================] - 5s 207us/step - loss: 1.0953 - acc: 0.9314 0s - loss: 1.0971 - acc: 0.93 - ETA: 0s - loss: 1.1019\n",
      "Epoch 5/5\n",
      "22400/22400 [==============================] - 5s 210us/step - loss: 1.0973 - acc: 0.9310  - ETA:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-21 19:36:27,340 : INFO : END GRID SEARCH\n",
      "2018-10-21 19:36:27,341 : INFO : END OF GRID SEARCH\n",
      "2018-10-21 19:36:27,341 : INFO : PRINTING RESULTS\n",
      "2018-10-21 19:36:27,342 : INFO : Best: -0.073637 using {'batch_size': 5, 'epochs': 5, 'optimizer': 'SGD'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean_fit_time', 'mean_score_time', 'mean_test_RMS', 'mean_train_RMS', 'param_batch_size', 'param_epochs', 'param_optimizer', 'params', 'rank_test_RMS', 'split0_test_RMS', 'split0_train_RMS', 'split1_test_RMS', 'split1_train_RMS', 'split2_test_RMS', 'split2_train_RMS', 'std_fit_time', 'std_score_time', 'std_test_RMS', 'std_train_RMS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-21 19:36:27,407 : INFO : SAVED BEST MODEL IN best_model_grid_NN.h5\n",
      "2018-10-21 19:36:27,409 : INFO : EXECUTED IN 140.697424 SEC\n",
      "2018-10-21 19:36:27,411 : INFO : END\n"
     ]
    }
   ],
   "source": [
    "grid_result = main('RFC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Warning Exercise  \n",
    "\n",
    "On the the code chunk above there are a couple a consideration to make:\n",
    "\n",
    "  1. **Parallelization**: note how the parallelization it's handled. Is there something to consider (hint: `n_jobs`) \n",
    "  2. **Parameters**: for the **NN** take note of what are the parameters of the model and what is the results. Is there something to notice? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "Autoencoders are the half way between *supervised* and *unsupervised* learning: the input and the output are the same, so the networking it's learning from itself.\n",
    "\n",
    "For this reason they are called **self-supervised**.\n",
    "\n",
    "From the [man page](https://blog.keras.io/building-autoencoders-in-keras.html) of Keras:\n",
    "\n",
    "*Autoencoding is a data compression algorithm where the compression and decompression functions are 1) data-specific, 2) lossy, and 3) learned automatically from examples rather than engineered by a human. Additionally, in almost all contexts where the term \"autoencoder\" is used, the compression and decompression functions are implemented with neural networks.*\n",
    "\n",
    "  1. *Autoencoders are data-specific, which means that they will only be able to compress data similar to what they have been trained on. This is different from, say, the MPEG-2 Audio Layer III (MP3) compression algorithm, which only holds assumptions about \"sound\" in general, but not about specific types of sounds. An autoencoder trained on pictures of faces would do a rather poor job of compressing pictures of trees, because the features it would learn would be face-specific.*\n",
    "  2. *Autoencoders are lossy, which means that the decompressed outputs will be degraded compared to the original inputs (similar to MP3 or JPEG compression). This differs from lossless arithmetic compression.*\n",
    "  3. *Autoencoders are learned automatically from data examples, which is a useful property: it means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn't require any new engineering, just appropriate training data.*\n",
    "  \n",
    "What autoencoders are good for:\n",
    "\n",
    "  - Data Denoising\n",
    "  - Dimension Reduction\n",
    "  - Data Visualization (basically the same as 2, but plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, SVG\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from datetime import datetime\n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('')\n",
    "\n",
    "encoding_dim = 32\n",
    "num_images = 10\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "class CreateNN:\n",
    "    def __init__(self,**kargs):\n",
    "        self.input_dim = kargs['id']\n",
    "        self.compression_factor = float(self.input_dim) / encoding_dim\n",
    "        logger.info(\"COMPRESSING FACTOR: %s\",str(self.compression_factor))\n",
    "        self.actFun = 'relu'\n",
    "        self.epochs = 5\n",
    "        self.batch_size = 256\n",
    "        self.x_train = kargs['xt']\n",
    "        self.x_test =kargs['xte']\n",
    "        \n",
    "    def createAutoEncoder(self):\n",
    "        self.autoencoder = Sequential()\n",
    "        self.autoencoder.add(Dense(encoding_dim, input_shape=(self.input_dim,), activation=self.actFun))\n",
    "        self.autoencoder.add(Dense(self.input_dim, activation='sigmoid'))\n",
    "        logger.info('SUMMARY OF THE AUTOENCODER MODEL')\n",
    "        print(self.autoencoder.summary())\n",
    "        \n",
    "        \n",
    "    def layerEncoder(self):\n",
    "        #Cration of a tensor\n",
    "        input_img = Input(shape=(self.input_dim,))\n",
    "        logger.info('LAYERS OF THE MODEL')\n",
    "        print(self.autoencoder.layers)\n",
    "        logger.info('SELECTING THE FIRST LAYER')\n",
    "        encoder_layer = self.autoencoder.layers[0]\n",
    "        self.encoder = Model(input_img, encoder_layer(input_img))#input and output \n",
    "        logger.info('SUMMARY OF THE ENCODER MODEL')\n",
    "        print(self.encoder.summary())\n",
    "    \n",
    "    def modelFit(self):\n",
    "        self.autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "        self.autoencoder.fit(self.x_train, self.x_train,\n",
    "                epochs=self.epochs,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                validation_split=0.10)#validation_data=(self.x_test, self.x_test)\n",
    "        return self.autoencoder\n",
    "    \n",
    "    def modelPredict(self):\n",
    "        encoded_imgs = self.encoder.predict(self.x_test)\n",
    "        decoded_imgs = self.autoencoder.predict(self.x_test)\n",
    "        return encoded_imgs,decoded_imgs\n",
    "\n",
    "class PlotResuts:\n",
    "    def __init__(self,**kargs):\n",
    "        self.random_test_images = kargs['ri']\n",
    "        self.num_images =kargs['ni']\n",
    "        self.x_test = kargs['x']\n",
    "        self.decoded_imgs = kargs['di']\n",
    "        self.encoded_imgs = kargs['ei']\n",
    "        plt.figure(figsize=(18, 4))\n",
    "    \n",
    "    def plotAll(self):\n",
    "        logger.info('START LOOP OVER TEST IMAGES')\n",
    "        for i, self.image_idx in enumerate(self.random_test_images):\n",
    "            # plot original image\n",
    "            self.plotOne(self.x_test,i + 1,28,28)\n",
    "            # plot encoded\n",
    "            self.plotOne(self.encoded_imgs,num_images + i + 1,8,4)\n",
    "            # plot decoded\n",
    "            self.plotOne(self.decoded_imgs,2*num_images + i + 1,28,28)\n",
    "        plt.show()\n",
    "        \n",
    "    def plotOne(self,x,sec_dim,shape1,shape2):\n",
    "            ax = plt.subplot(3,self.num_images,sec_dim)\n",
    "            plt.imshow(x[self.image_idx].reshape(shape1, shape2))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "\n",
    "            \n",
    "def prepareData():\n",
    "    # Loads the training and test data sets (ignoring class labels)\n",
    "    (x_train, _), (x_test, _) = mnist.load_data()\n",
    "    # Scales the training and test data to range between 0 and 1.\n",
    "    max_value = float(x_train.max())\n",
    "    x_train = x_train.astype('float32') / max_value\n",
    "    x_test = x_test.astype('float32') / max_value\n",
    "    x_train.shape, x_test.shape\n",
    "    x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "    x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "    #(x_train.shape, x_test.shape)\n",
    "    input_dim = x_train.shape[1]\n",
    "    return x_train, x_test,input_dim\n",
    "\n",
    "def main():\n",
    "    start = datetime.now()\n",
    "    logger.info('START')\n",
    "    logger.info('PREPARATION OF THE DATE')\n",
    "    x_train,x_test,input_dim = prepareData()\n",
    "    logger.info('SELECTION OT TEST IMAGES')\n",
    "    random_test_images = np.random.randint(x_test.shape[0], size=num_images)\n",
    "    logger.info('CREATION OF THE CLASS FOR THE MODEL')\n",
    "    cnn = CreateNN(id=input_dim,ri=random_test_images,xt=x_train,xte=x_test)\n",
    "    logger.info('AUTOENCODER MODEL')\n",
    "    cnn.createAutoEncoder()\n",
    "    logger.info('ENCODER LAYER')\n",
    "    cnn.layerEncoder()\n",
    "    logger.info('FIT OF THE MODEL')\n",
    "    cnn.modelFit()\n",
    "    logger.info('PREDICTION OVER TEST')\n",
    "    encoded_imgs, decoded_imgs = cnn.modelPredict()\n",
    "    logger.info('PREPARING THE CLASS FOR PRINTING')\n",
    "    pr = PlotResuts(ri=random_test_images,ni=num_images,x=x_test,di=decoded_imgs,ei=encoded_imgs)\n",
    "    logger.info('PLOTTING ALL')\n",
    "    pr.plotAll()\n",
    "    logger.info(\"EXECUTED IN %f SEC\"%((datetime.now()-start)).total_seconds())\n",
    "    logger.info(\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "a = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "a[0][0][0]\n",
    "a[0][1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise \n",
    "\n",
    "For the code below:\n",
    "\n",
    "  - Give an *high level* description of what the code does\n",
    "  - Give a *line by line* description of the methodos `makeJoin` and `preProcess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.cluster import KMeans\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('')\n",
    "\n",
    "dataFolder = 'data'\n",
    "chifData = 'chifdata'\n",
    "prospFile = 'all_prospects_france.csv'\n",
    "\n",
    "\n",
    "class MakeClusters:\n",
    "    def __init__(self,**kargs):\n",
    "        self.outFile = kargs['fo']\n",
    "        self.numClus = kargs['nk']\n",
    "        self.numEmp = kargs['ne']\n",
    "        self.siren = kargs['si']\n",
    "        self.to = kargs['to']\n",
    "\n",
    "\n",
    "\n",
    "    def makeClusters(self,df):\n",
    "        logger.info('Defining KMeans with %i clusters',self.numClus)\n",
    "        kmeans = KMeans(n_clusters=self.numClus)\n",
    "        kmeans.fit(df)\n",
    "        labels = kmeans.predict(df)\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        logger.info('Adding labels to the dataframes')\n",
    "        df['labels'] = labels\n",
    "        df['Résultat 1'] = self.to\n",
    "        df['Siren'] = self.siren\n",
    "        return df\n",
    "\n",
    "    def writeOutFile(self,df):\n",
    "        logger.info('Writing out file to %s',self.outFile)\n",
    "        df.to_csv(self.outFile,index=False)\n",
    "\n",
    "\n",
    "class PrepareData:\n",
    "    def __init__(self, **kargs):\n",
    "        self.inFile = kargs['fi']\n",
    "        self.inDir = kargs['di']\n",
    "        self.numEmp = kargs['ne']\n",
    "\n",
    "    def readOne(self):\n",
    "        logger.info('Reading data from %s',self.inFile)\n",
    "        self.dfPros = pd.read_csv(self.inFile,sep=',')\n",
    "        print('columns for prof',self.dfPros.columns,self.dfPros.shape,self.dfPros.dtypes)\n",
    "\n",
    "    def readMultiple(self):\n",
    "        self.dfInfo = pd.DataFrame()\n",
    "        for filename in os.listdir(self.inDir):\n",
    "            fi = \"%s/%s\"%(self.inDir,filename)\n",
    "            logger.info('Readin file %s',fi)\n",
    "            curdf = pd.read_csv(fi,sep=';',usecols=['Siren','Résultat 1'])\n",
    "            self.dfInfo = self.dfInfo.append(curdf)\n",
    "        print('columns for info',self.dfInfo.columns,self.dfInfo.shape,self.dfInfo.dtypes)\n",
    "\n",
    "    def makeJoin(self):\n",
    "        logger.info('Making the join')\n",
    "        self.dfJoin = self.dfPros.set_index('SIREN').join(self.dfInfo.set_index('Siren'),how='inner')\n",
    "        self.dfJoin.reset_index(inplace=True)\n",
    "        print('shape after join:',self.dfJoin.shape,self.dfJoin.columns)\n",
    "        return self.dfJoin\n",
    "\n",
    "    def preProcess(self,df):\n",
    "        df.dropna(inplace=True)\n",
    "        logger.info('Removing rows with more that %i employes',self.numEmp)\n",
    "        df = df.loc[df['eff_category']<=self.numEmp]\n",
    "        logger.info('SCALING VARIABILES')\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        np_scaled = min_max_scaler.fit_transform(pd.DataFrame(df.loc[:,'Résultat 1']))\n",
    "        df['to_norm'] = np_scaled\n",
    "        to = df[['Résultat 1']]\n",
    "        siren = df[['index']]\n",
    "        df.drop(['Résultat 1','index'],axis=1,inplace=True)\n",
    "        return df, to, siren\n",
    "\n",
    "def main():\n",
    "    start=datetime.now()\n",
    "    logger.info(\"START\")\n",
    "    inFile = \"%s/%s\"%(dataFolder,prospFile)\n",
    "    inDir = \"%s/%s\"%(dataFolder,chifData)\n",
    "    numClus = 5\n",
    "    outFile = 'testClustering.csv'\n",
    "    numEmp = 10\n",
    "    logger.info(\"RUNNING WITH INFILE %s, OUTFILE %s , NUM EMPLOIES %d AND NUM OF CLUSTER %d\",inFile,outFile,numEmp,numClus)\n",
    "    logger.info('INSTANTIATE PREPAREDATA')\n",
    "    pdt = PrepareData(fi=inFile,di=inDir,ne=numEmp)\n",
    "    pdt.readOne()\n",
    "    pdt.readMultiple()\n",
    "    df = pdt.makeJoin()\n",
    "    df,to,siren = pdt.preProcess(df)\n",
    "    logger.info('INSTANTIATE MAKECLUSTER')\n",
    "    mc = MakeClusters(fo=outFile,nk=numClus,ne=numEmp,si=siren,to=to)\n",
    "    df = mc.makeClusters(df)\n",
    "    mc.writeOutFile(df)\n",
    "    logger.info('DONE IN %s',str(datetime.now()-start))\n",
    "    logger.info('END')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
